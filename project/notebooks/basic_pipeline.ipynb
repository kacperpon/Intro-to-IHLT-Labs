{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHLT Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from features.statistical_features import FeatureExtractor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../data/train/01_raw/'\n",
    "TRAIN_GS_PATH = '../data/train/scores/'\n",
    "TEST_PATH = '../data/test/01_raw/'\n",
    "TEST_GS_PATH = '../data/test/scores/'\n",
    "TRAIN_SAVE_PATH = '../data/train/02_preprocessed/preprocessed_train_data.csv'\n",
    "TEST_SAVE_PATH = '../data/test/02_preprocessed/preprocessed_test_data.csv'\n",
    "PREDICTED_SAVE_PATH = '../data/test/03_predicted/'\n",
    "\n",
    "def load_data(path_f, path_gs, files):\n",
    "    # Read first file\n",
    "    dt = pd.read_csv(path_f + 'STS.input.' + files[0] + '.txt', sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['s1', 's2'])\n",
    "    dt['gs'] = pd.read_csv(path_gs + 'STS.gs.' + files[0] + '.txt', sep='\\t', header=None, names=['gs'])\n",
    "    dt['file']=files[0]\n",
    "    # Concatenate the rest of files\n",
    "    for f in files[1:]:\n",
    "        dt2 = pd.read_csv(path_f + 'STS.input.' + f + '.txt', sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['s1', 's2'])\n",
    "        dt2['gs'] = pd.read_csv(path_gs + 'STS.gs.' + f + '.txt', sep='\\t', header=None, names=['gs'])\n",
    "        dt2['file']=f\n",
    "        dt = pd.concat([dt, dt2], ignore_index=True)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN(df, input, output):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)\n",
    "    model = Sequential()\n",
    "    # model.add(Dense(10, input_dim=len(input), activation='relu'))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dense(5, activation='relu'))               \n",
    "    model.add(Dense(5, activation='relu'))               \n",
    "    model.add(Dense(1))                                   \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32, verbose=0)\n",
    "    return history, model\n",
    "\n",
    "def train_MLP(df, input, output):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=(5, 5, 5), \n",
    "        max_iter=500, \n",
    "        random_state=42, \n",
    "        early_stopping=True, \n",
    "        validation_fraction=0.1,  \n",
    "        verbose=True\n",
    "    )\n",
    "    history = model.fit(X, y)\n",
    "    return history, model\n",
    "\n",
    "def train_random_forest(df, input, output):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "\n",
    "    # Model with additional parameters - TODO: Grid search for hyperparameters\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=200,          \n",
    "        max_depth=10,              \n",
    "        min_samples_split=5,      \n",
    "        min_samples_leaf=2,        \n",
    "        max_features='sqrt',      \n",
    "        bootstrap=True,           \n",
    "    )\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def train_svr(df, input, output, kernel='rbf', C=1.0, epsilon=0.1):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "    \n",
    "    model = SVR(kernel=kernel, C=C, epsilon=epsilon)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_svr_with_grid_search(df, input, output):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "    \n",
    "    model = SVR()\n",
    "    \n",
    "    param_grid = {\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'degree': [2, 3, 4] \n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=10)\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best params:\", best_params)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(history.history['mae'])\n",
    "    plt.plot(history.history['val_mae'])\n",
    "    plt.title('model mae')\n",
    "    plt.ylabel('mae')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pipeline\n",
    "\n",
    "### Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data\n",
      "Adding POS based features...\n",
      "Adding synset based features...\n",
      "Processed 2234/2234 rows (100.0%)    \n",
      "Loading test data\n",
      "Adding POS based features...\n",
      "Adding synset based features...\n",
      "Processed 3108/3108 rows (100.0%)    \n",
      "Train and test datasets ready\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Create the desired features\n",
    "def add_features(dt):\n",
    "    feature_extractor.add_POS_statistics(dt)\n",
    "    feature_extractor.add_synset_statistics_ext(dt)\n",
    "    feature_extractor.add_lemma_statistics(dt)\n",
    "\n",
    "# Load train data\n",
    "print('Loading train data')\n",
    "all_train_files = ['SMTeuroparl', 'MSRvid', 'MSRpar']\n",
    "df_train = load_data(TRAIN_PATH, TRAIN_GS_PATH, all_train_files)\n",
    "\n",
    "# Add features to the train data\n",
    "add_features(df_train)\n",
    "\n",
    "# Save df to a file\n",
    "df_train.to_csv(TRAIN_SAVE_PATH, index=False)\n",
    "\n",
    "# Load test data\n",
    "print('Loading test data')\n",
    "all_test_files = ['SMTeuroparl', 'MSRvid', 'MSRpar', 'surprise.OnWN', 'surprise.SMTnews']\n",
    "df_test = load_data(TEST_PATH, TEST_GS_PATH, all_test_files)\n",
    "\n",
    "# Add the features to the test data\n",
    "add_features(df_test)\n",
    "\n",
    "# Save df_test to a file\n",
    "df_test.to_csv(TEST_SAVE_PATH, index=False)\n",
    "\n",
    "print('Train and test datasets ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Word based features...\n",
      "[('but', 'CC'), ('they', 'PRP'), ('were', 'VBD'), ('necessary', 'JJ'), ('.', '.')]\n",
      "[('necessary', 'JJ')]\n",
      "[('but', 'CC'), ('they', 'PRP'), ('were', 'VBD'), ('needed', 'VBN'), ('.', '.')]\n",
      "[('needed', 'VBN')]\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor()\n",
    "feature_extractor.add_Word_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>gs</th>\n",
       "      <th>file</th>\n",
       "      <th>s1_n_words</th>\n",
       "      <th>s2_n_words</th>\n",
       "      <th>s1_n_verbs_tot</th>\n",
       "      <th>s2_n_verbs_tot</th>\n",
       "      <th>s1_n_verbs_pres</th>\n",
       "      <th>s2_n_verbs_pres</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_lemma_similarity</th>\n",
       "      <th>max_lemma_similarity</th>\n",
       "      <th>shared_lemma_count</th>\n",
       "      <th>dice_coefficient</th>\n",
       "      <th>lemma_bigram_overlap</th>\n",
       "      <th>lemma_lcs_length</th>\n",
       "      <th>lemma_edit_distance</th>\n",
       "      <th>proportion_s1_in_s2</th>\n",
       "      <th>proportion_s2_in_s1</th>\n",
       "      <th>lemma_position_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The leaders have now been given a new chance a...</td>\n",
       "      <td>The leaders benefit aujourd' hui of a new luck...</td>\n",
       "      <td>4.50</td>\n",
       "      <td>SMTeuroparl</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amendment No 7 proposes certain changes in the...</td>\n",
       "      <td>Amendment No 7 is proposing certain changes in...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>SMTeuroparl</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299639</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Let me remind you that our allies include ferv...</td>\n",
       "      <td>I would like to remind you that among our alli...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>SMTeuroparl</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197031</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The vote will take place today at 5.30 p.m.</td>\n",
       "      <td>The vote will take place at 5.30pm</td>\n",
       "      <td>4.50</td>\n",
       "      <td>SMTeuroparl</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268684</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
       "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>SMTeuroparl</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  s1  \\\n",
       "0  The leaders have now been given a new chance a...   \n",
       "1  Amendment No 7 proposes certain changes in the...   \n",
       "2  Let me remind you that our allies include ferv...   \n",
       "3        The vote will take place today at 5.30 p.m.   \n",
       "4  The fishermen are inactive, tired and disappoi...   \n",
       "\n",
       "                                                  s2    gs         file  \\\n",
       "0  The leaders benefit aujourd' hui of a new luck...  4.50  SMTeuroparl   \n",
       "1  Amendment No 7 is proposing certain changes in...  5.00  SMTeuroparl   \n",
       "2  I would like to remind you that among our alli...  4.25  SMTeuroparl   \n",
       "3                 The vote will take place at 5.30pm  4.50  SMTeuroparl   \n",
       "4  The fishermen are inactive, tired and disappoi...  5.00  SMTeuroparl   \n",
       "\n",
       "   s1_n_words  s2_n_words  s1_n_verbs_tot  s2_n_verbs_tot  s1_n_verbs_pres  \\\n",
       "0         8.0        10.0             4.0             3.0              0.0   \n",
       "1         6.0         6.0             2.0             2.0              2.0   \n",
       "2         7.0         7.0             3.0             1.0              1.0   \n",
       "3         5.0         3.0             1.0             1.0              0.0   \n",
       "4         4.0         4.0             2.0             2.0              1.0   \n",
       "\n",
       "   s2_n_verbs_pres  ...  avg_lemma_similarity  max_lemma_similarity  \\\n",
       "0              1.0  ...              0.203041                   1.0   \n",
       "1              2.0  ...              0.299639                   1.0   \n",
       "2              0.0  ...              0.197031                   1.0   \n",
       "3              0.0  ...              0.268684                   1.0   \n",
       "4              1.0  ...              0.522319                   1.0   \n",
       "\n",
       "   shared_lemma_count  dice_coefficient  lemma_bigram_overlap  \\\n",
       "0                   4          0.470588              0.000000   \n",
       "1                   6          0.857143              0.500000   \n",
       "2                   3          0.428571              0.000000   \n",
       "3                   3          0.600000              0.333333   \n",
       "4                   4          1.000000              1.000000   \n",
       "\n",
       "   lemma_lcs_length  lemma_edit_distance  proportion_s1_in_s2  \\\n",
       "0                 4                    6             0.500000   \n",
       "1                 6                    1             0.857143   \n",
       "2                 3                    6             0.428571   \n",
       "3                 3                    3             0.500000   \n",
       "4                 4                    0             1.000000   \n",
       "\n",
       "   proportion_s2_in_s1  lemma_position_similarity  \n",
       "0             0.444444                   0.850000  \n",
       "1             0.857143                   1.000000  \n",
       "2             0.428571                   0.857143  \n",
       "3             0.750000                   1.000000  \n",
       "4             1.000000                   1.000000  \n",
       "\n",
       "[5 rows x 77 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataframes from files\n",
    "df_train = pd.read_csv(TRAIN_SAVE_PATH)\n",
    "df_test = pd.read_csv(TEST_SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to use in the model\n",
    "features = [\n",
    "            # 's1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_nouns', 's1_n_adjectives', 's1_n_adverbs', \n",
    "            # 's2_n_words', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_nouns', 's2_n_adjectives', 's2_n_adverbs', \n",
    "            # 'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', \n",
    "            \n",
    "            'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "\n",
    "            'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity',\n",
    "            'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity',\n",
    "            'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity',\n",
    "            'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity',\n",
    "            'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity',\n",
    "\n",
    "            # 'lemma_diversity', 'shared_lemmas_ratio', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient',\n",
    "            # 'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity'\n",
    "            ]\n",
    "\n",
    "\n",
    "# Train a NN\n",
    "# hist, model = train_NN(df, features, 'gs')\n",
    "# plot_history(hist)\n",
    "\n",
    "# Train a MLP\n",
    "# hist, model = train_MLP(df, features, 'gs')\n",
    "\n",
    "# Train a Random Forest\n",
    "model = train_random_forest(df_train, features, 'gs')\n",
    "\n",
    "# Train a Support vector regression\n",
    "# model = train_svr(df, features, 'gs')\n",
    "\n",
    "# Find best svr\n",
    "# model = train_svr_with_grid_search(df, features, 'gs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute correlation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation: 0.7120224907857646\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Fill column of the dataset with the predictions of the model\n",
    "df_test['predicted'] = model.predict(df_test[features])\n",
    "\n",
    "# Compute the Pearson correlation between the predictions and the gold standard\n",
    "corr = pearsonr(df_test['gs'], df_test['predicted'])[0]\n",
    "print('Pearson correlation:', corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paubl\\AppData\\Local\\Temp\\ipykernel_24024\\2052023131.py:21: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df.applymap(clean_value)\n"
     ]
    }
   ],
   "source": [
    "# Save the predicted dataset\n",
    "# Add timestamp to the name of the file\n",
    "import re\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "df_test.to_csv(PREDICTED_SAVE_PATH + timestamp + '_predicted_test_data.csv', index=False)\n",
    "\n",
    "\n",
    "def clean_illegal_characters(df):\n",
    "    # Definir una expressió regular per trobar caràcters il·legals\n",
    "    illegal_characters_re = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "    \n",
    "    # Funció per netejar un valor individual\n",
    "    def clean_value(value):\n",
    "        if isinstance(value, str):\n",
    "            return illegal_characters_re.sub('', value)\n",
    "        return value\n",
    "    \n",
    "    # Aplicar la funció de neteja a cada cel·la del DataFrame\n",
    "    return df.applymap(clean_value)\n",
    "\n",
    "df_clean = clean_illegal_characters(df_test)\n",
    "df_clean.to_excel(PREDICTED_SAVE_PATH + timestamp + '_predicted_test_data.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Pearson correlation: 0.7114133972203263\n",
      "RF Pearson correlation: 0.7089882260352499\n",
      "RF Pearson correlation: 0.7128500076042451\n",
      "RF Pearson correlation: 0.7142873108528178\n",
      "RF Pearson correlation: 0.7123363771115678\n",
      "RF Pearson correlation: 0.7155416859979122\n",
      "RF Pearson correlation: 0.7129274303810641\n",
      "RF Pearson correlation: 0.7142129979242231\n",
      "RF Pearson correlation: 0.7130897149177514\n",
      "RF Pearson correlation: 0.7141304458031449\n",
      "RF Pearson correlation: 0.7129777593848302\n"
     ]
    }
   ],
   "source": [
    "N_ITERS = 10\n",
    "nn_p = 0\n",
    "rf_p = 0\n",
    "\n",
    "for i in range(N_ITERS):\n",
    "    # print('Iteration:', i)\n",
    "    # hist, model = train_NN(df, features, 'gs')\n",
    "    # df_test['predicted'] = model.predict(df_test[features])\n",
    "    # corr = pearsonr(df_test['gs'], df_test['predicted'])[0]\n",
    "    # print('NN Pearson correlation:', corr)\n",
    "    # nn_p += corr\n",
    "\n",
    "    model = train_random_forest(df_train, features, 'gs')\n",
    "    df_test['predicted'] = model.predict(df_test[features])\n",
    "    corr = pearsonr(df_test['gs'], df_test['predicted'])[0]\n",
    "    print('RF Pearson correlation:', corr)\n",
    "    rf_p += corr\n",
    "\n",
    "# print('NN Pearson correlation:', nn_p/N_ITERS)\n",
    "print('RF Pearson correlation:', rf_p/N_ITERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
