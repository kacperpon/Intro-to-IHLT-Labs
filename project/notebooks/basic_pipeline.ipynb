{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHLT Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from features.statistical_features import FeatureExtractor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../data/train/01_raw/'\n",
    "TRAIN_GS_PATH = '../data/train/scores/'\n",
    "TEST_PATH = '../data/test/01_raw/'\n",
    "TEST_GS_PATH = '../data/test/scores/'\n",
    "TRAIN_SAVE_PATH = '../data/train/02_preprocessed/preprocessed_train_data.csv'\n",
    "TEST_SAVE_PATH = '../data/test/02_preprocessed/preprocessed_test_data.csv'\n",
    "PREDICTED_SAVE_PATH = '../data/test/03_predicted/'\n",
    "\n",
    "def load_data(path_f, path_gs, files):\n",
    "    # Read first file\n",
    "    dt = pd.read_csv(path_f + 'STS.input.' + files[0] + '.txt', sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['s1', 's2'])\n",
    "    dt['gs'] = pd.read_csv(path_gs + 'STS.gs.' + files[0] + '.txt', sep='\\t', header=None, names=['gs'])\n",
    "    dt['file']=files[0]\n",
    "    # Concatenate the rest of files\n",
    "    for f in files[1:]:\n",
    "        dt2 = pd.read_csv(path_f + 'STS.input.' + f + '.txt', sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['s1', 's2'])\n",
    "        dt2['gs'] = pd.read_csv(path_gs + 'STS.gs.' + f + '.txt', sep='\\t', header=None, names=['gs'])\n",
    "        dt2['file']=f\n",
    "        dt = pd.concat([dt, dt2], ignore_index=True)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy.stats as st\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def create_model(input_dim, learning_rate=0.001, neurons=10, hidden_layers=2):\n",
    "    \"\"\"\n",
    "    Function to create a Keras model with specified hyperparameters.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=input_dim, activation='relu'))  # Input layer\n",
    "    for _ in range(hidden_layers):  # Add hidden layers\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def train_NN(df, input, output):\n",
    "    \"\"\"\n",
    "    Train a neural network with grid search to find the best hyperparameters.\n",
    "    \"\"\"\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    # Wrap Keras model for compatibility with GridSearchCV\n",
    "    model = KerasRegressor(\n",
    "        model=create_model,\n",
    "        model__input_dim=input_dim,  \n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    pearson_score = make_scorer(pearson_scorer, greater_is_better=True)\n",
    "\n",
    "    # TODO: Run this grid with beefier hardware, it's too slow for me to test\n",
    "    # Define hyperparameter grid\n",
    "    # param_grid = {\n",
    "    #     'batch_size': [16, 32, 64],\n",
    "    #     'epochs': [50, 100],\n",
    "    #     'learning_rate': [0.001, 0.01],\n",
    "    #     'neurons': [5, 10, 20],\n",
    "    #     'hidden_layers': [2, 3, 4]\n",
    "    # }\n",
    "\n",
    "    param_grid = {\n",
    "        \"model__neurons\": [5, 10],\n",
    "        \"model__hidden_layers\": [2],\n",
    "        \"model__learning_rate\": [0.001],\n",
    "        \"batch_size\": [16, 32],\n",
    "        \"epochs\": [50, 100],\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=pearson_score,\n",
    "        cv=3,\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def train_MLP(df, input, output):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "\n",
    "    # Define the base model\n",
    "    model = MLPRegressor(max_iter=1000, early_stopping=True, validation_fraction=0.1, verbose=False)\n",
    "\n",
    "    pearson_score = make_scorer(pearson_scorer, greater_is_better=True)\n",
    "\n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(10,), (50,), (100,), (50, 100)],  # Vary number and size of layers\n",
    "        'activation': ['relu', 'tanh'],                                       # Activation functions\n",
    "        'solver': ['adam'],                                            # Optimization solvers\n",
    "        'alpha': [0.0001, 0.001, 0.01],                                       # Regularization strength\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'max_iter': [200, 500, 1000]                                                      # Learning rate schedules\n",
    "    }\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=pearson_score,  \n",
    "        cv=5,                              # 3-fold cross-validation\n",
    "        verbose=2,                         # Display progress\n",
    "        n_jobs=-1,                          # Use all available cores\n",
    "    )\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    results = grid_search.cv_results_\n",
    "    for mean_score, params in zip(results['mean_test_score'], results['params']):\n",
    "        if np.isnan(mean_score):\n",
    "            print(\"Failed combination:\", params)\n",
    "\n",
    "    # Best model from grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def pearson_scorer(y_true, y_pred):\n",
    "    # pearsonr returns a tuple (correlation, p-value)\n",
    "    return pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "def train_RF(df, input, output):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "\n",
    "    # Define the model\n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    # TODO: Run this grid with beefier hardware, it's too slow for me to test\n",
    "    # Define the grid of hyperparameters to search\n",
    "    # param_grid = {\n",
    "    #     'n_estimators': [100, 200, 300],  \n",
    "    #     'max_depth': [None, 10, 20, 30], \n",
    "    #     'min_samples_split': [2, 5, 10], \n",
    "    #     'min_samples_leaf': [1, 2, 4],   \n",
    "    #     'max_features': ['sqrt', 'log2'],\n",
    "    #     'bootstrap': [True, False],      \n",
    "    # }\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],  \n",
    "        'max_depth': [None, 10], \n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],  \n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'bootstrap': [True, False],     \n",
    "    }\n",
    "\n",
    "    pearson_score = make_scorer(pearson_scorer, greater_is_better=True)\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=pearson_score,  \n",
    "        cv=5,                              \n",
    "        verbose=2,                         \n",
    "        n_jobs=-1                          \n",
    "    )\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Best model from grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "    return best_model, grid_search.best_params_\n",
    "\n",
    "# TODO: No longer slow but extremely poor performance, investigate\n",
    "def train_SVR(df, input, output):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    pearson_score = make_scorer(pearson_scorer, greater_is_better=True)\n",
    "\n",
    "    model = SVR(cache_size=2000)  # cache kernel computations\n",
    "\n",
    "    # Reduced / randomized parameter space\n",
    "    param_grid = {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,         # fewer random samples\n",
    "        cv=3,              # fewer folds\n",
    "        scoring=pearson_score,\n",
    "        verbose=1,\n",
    "        n_jobs=10,\n",
    "    )\n",
    "\n",
    "    random_search.fit(X_scaled, y)\n",
    "    best_params = random_search.best_params_\n",
    "    print(\"Best params:\", best_params)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def train_single_RF(df, input, output, params):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "\n",
    "    # Define the model\n",
    "    pearson_score = make_scorer(pearson_scorer, greater_is_better=True)\n",
    "    model = RandomForestRegressor(**params) #, scoring=pearson_score)\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(history.history['mae'])\n",
    "    plt.plot(history.history['val_mae'])\n",
    "    plt.title('model mae')\n",
    "    plt.ylabel('mae')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pipeline\n",
    "\n",
    "### Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data\n",
      "Adding POS based features...\n",
      "Adding synset based features...\n",
      "Processed 2234/2234 rows (100.0%)    \n",
      "Adding lemma based features...\n",
      "Loading test data\n",
      "Adding POS based features...\n",
      "Adding synset based features...\n",
      "Processed 3108/3108 rows (100.0%)    \n",
      "Adding lemma based features...\n",
      "Train and test datasets ready\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Create the desired features\n",
    "def add_features(dt):\n",
    "    feature_extractor.add_POS_statistics(dt)\n",
    "    feature_extractor.add_synset_statistics_ext(dt)\n",
    "    feature_extractor.add_lemma_statistics(dt)\n",
    "\n",
    "# Load train data\n",
    "print('Loading train data')\n",
    "all_train_files = ['SMTeuroparl', 'MSRvid', 'MSRpar']\n",
    "df_train = load_data(TRAIN_PATH, TRAIN_GS_PATH, all_train_files)\n",
    "\n",
    "# Add features to the train data\n",
    "add_features(df_train)\n",
    "\n",
    "# Save df to a file\n",
    "df_train.to_csv(TRAIN_SAVE_PATH, index=False)\n",
    "\n",
    "# Load test data\n",
    "print('Loading test data')\n",
    "all_test_files = ['SMTeuroparl', 'MSRvid', 'MSRpar', 'surprise.OnWN', 'surprise.SMTnews']\n",
    "df_test = load_data(TEST_PATH, TEST_GS_PATH, all_test_files)\n",
    "\n",
    "# Add the features to the test data\n",
    "add_features(df_test)\n",
    "\n",
    "# Save df_test to a file\n",
    "df_test.to_csv(TEST_SAVE_PATH, index=False)\n",
    "\n",
    "print('Train and test datasets ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor()\n",
    "#feature_extractor.add_Word_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframes from files\n",
    "df_train = pd.read_csv(TRAIN_SAVE_PATH)\n",
    "df_test = pd.read_csv(TEST_SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# Features to use in the model\n",
    "features = [\n",
    "            's1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_nouns', 's1_n_adjectives', 's1_n_adverbs', \n",
    "            's2_n_words', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_nouns', 's2_n_adjectives', 's2_n_adverbs', \n",
    "            'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', \n",
    "            \n",
    "            'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "\n",
    "            'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity',\n",
    "            'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity',\n",
    "            'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity',\n",
    "            'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity',\n",
    "            'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity',\n",
    "\n",
    "            'lemma_diversity', 'shared_lemmas_ratio', 'lemma_jackard_similarity', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient',\n",
    "            'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity'\n",
    "            ]\n",
    "\n",
    "\n",
    "# Train a NN\n",
    "# model = train_NN(df_train, features, 'gs')\n",
    "# plot_history(hist)\n",
    "\n",
    "# Train a MLP\n",
    "#best_mlp_model = train_MLP(df_train, features, 'gs')\n",
    "\n",
    "# Train a Random Forest\n",
    "model, params = train_RF(df_train, features, 'gs')\n",
    "# Best params found: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
    "# model = train_single_RF(df_train, features, 'gs', params)\n",
    "\n",
    "# Train  an SVR\n",
    "# model = train_SVR(df_train, features, 'gs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute correlation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation: 0.7540496030246525\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Fill column of the dataset with the predictions of the model\n",
    "# df_test['predicted'] = best_rf_model.predict(df_test[features])\n",
    "#df_test['predicted'] = best_mlp_model.predict(df_test[features])\n",
    "df_test['predicted'] = model.predict(df_test[features])\n",
    "\n",
    "# Compute the Pearson correlation between the predictions and the gold standard\n",
    "corr = pearsonr(df_test['gs'], df_test['predicted'])[0]\n",
    "print('Pearson correlation:', corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paubl\\AppData\\Local\\Temp\\ipykernel_35124\\4206747572.py:18: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df.applymap(clean_value)\n"
     ]
    }
   ],
   "source": [
    "# Save the predicted dataset\n",
    "# Add timestamp to the name of the file\n",
    "import re\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "df_test.to_csv(PREDICTED_SAVE_PATH + timestamp + '_predicted_test_data.csv', index=False)\n",
    "\n",
    "\n",
    "def clean_illegal_characters(df):\n",
    "    illegal_characters_re = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "    \n",
    "    def clean_value(value):\n",
    "        if isinstance(value, str):\n",
    "            return illegal_characters_re.sub('', value)\n",
    "        return value\n",
    "    \n",
    "    return df.applymap(clean_value)\n",
    "\n",
    "df_clean = clean_illegal_characters(df_test)\n",
    "df_clean.to_excel(PREDICTED_SAVE_PATH + timestamp + '_predicted_test_data.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 RF Pearson correlation: 0.7551433485286597 => 0.7551433485286597\n",
      "1 RF Pearson correlation: 0.755104949541115 => 0.7551241490348873\n",
      "2 RF Pearson correlation: 0.7554078851901957 => 0.7552187277533235\n",
      "3 RF Pearson correlation: 0.7532854549570337 => 0.7547354095542511\n",
      "4 RF Pearson correlation: 0.7533938134124563 => 0.7544670903258922\n",
      "5 RF Pearson correlation: 0.7534126050655634 => 0.754291342782504\n",
      "6 RF Pearson correlation: 0.7510654619861754 => 0.7538305026687427\n",
      "7 RF Pearson correlation: 0.7550385904860417 => 0.7539815136459052\n",
      "8 RF Pearson correlation: 0.753789450172195 => 0.7539601732599374\n",
      "9 RF Pearson correlation: 0.7552045902018234 => 0.7540846149541259\n",
      "10 RF Pearson correlation: 0.755361727297691 => 0.7542007160762683\n",
      "11 RF Pearson correlation: 0.7536979282713231 => 0.7541588170925229\n",
      "12 RF Pearson correlation: 0.7536678058128037 => 0.7541210469940829\n",
      "13 RF Pearson correlation: 0.753927025512459 => 0.754107188316824\n",
      "14 RF Pearson correlation: 0.7551580325298214 => 0.7541772445976905\n",
      "15 RF Pearson correlation: 0.7549097707495759 => 0.7542230274821834\n",
      "16 RF Pearson correlation: 0.7542843104763741 => 0.7542266323641946\n",
      "17 RF Pearson correlation: 0.7558176375849919 => 0.7543150215431278\n",
      "18 RF Pearson correlation: 0.7544350797866435 => 0.7543213403980497\n",
      "19 RF Pearson correlation: 0.7553764827398657 => 0.7543740975151405\n",
      "20 RF Pearson correlation: 0.754966096493646 => 0.7544022879426884\n",
      "21 RF Pearson correlation: 0.754833332983501 => 0.754421880899089\n",
      "22 RF Pearson correlation: 0.7526666330983877 => 0.7543455657773194\n",
      "23 RF Pearson correlation: 0.7546841857265476 => 0.7543596749418705\n",
      "24 RF Pearson correlation: 0.7545964114058276 => 0.7543691444004288\n",
      "25 RF Pearson correlation: 0.7549936318949648 => 0.7543931631502188\n",
      "26 RF Pearson correlation: 0.7557742031771263 => 0.7544443127808449\n",
      "27 RF Pearson correlation: 0.7512779998686602 => 0.7543312301768382\n",
      "28 RF Pearson correlation: 0.7536877765518581 => 0.7543090421208044\n",
      "29 RF Pearson correlation: 0.7535116971077996 => 0.7542824639537042\n",
      "30 RF Pearson correlation: 0.7526316866099356 => 0.7542292130716471\n",
      "31 RF Pearson correlation: 0.7534581605702013 => 0.754205117680977\n",
      "32 RF Pearson correlation: 0.7539625794762714 => 0.7541977680384102\n",
      "33 RF Pearson correlation: 0.7535745364411898 => 0.7541794376973154\n",
      "34 RF Pearson correlation: 0.7561430958522268 => 0.7542355422160272\n",
      "35 RF Pearson correlation: 0.7526933635502413 => 0.7541927039197555\n",
      "36 RF Pearson correlation: 0.7524586033651697 => 0.7541458363371991\n",
      "37 RF Pearson correlation: 0.7532355693960229 => 0.7541218819440102\n",
      "38 RF Pearson correlation: 0.7544729542402736 => 0.7541308837977605\n",
      "39 RF Pearson correlation: 0.7552328631108665 => 0.7541584332805882\n",
      "40 RF Pearson correlation: 0.7552705938980134 => 0.7541855591493059\n",
      "41 RF Pearson correlation: 0.7561612535913513 => 0.7542325994931641\n",
      "42 RF Pearson correlation: 0.7557062377112242 => 0.7542668701493981\n",
      "43 RF Pearson correlation: 0.7536042163354161 => 0.7542518098354439\n",
      "44 RF Pearson correlation: 0.756141515519748 => 0.7542938032950952\n",
      "45 RF Pearson correlation: 0.752948371838627 => 0.754264554785172\n",
      "46 RF Pearson correlation: 0.7551040218079985 => 0.7542824157856578\n",
      "47 RF Pearson correlation: 0.7548016738765122 => 0.7542932336625506\n",
      "48 RF Pearson correlation: 0.7537281602083998 => 0.7542817015512413\n",
      "49 RF Pearson correlation: 0.7523330873648044 => 0.7542427292675126\n",
      "50 RF Pearson correlation: 0.7533042796517921 => 0.7542243282946554\n",
      "51 RF Pearson correlation: 0.7541961784692663 => 0.7542237869518593\n",
      "52 RF Pearson correlation: 0.7539683717915006 => 0.7542189677978903\n",
      "53 RF Pearson correlation: 0.7548298384008887 => 0.7542302802164643\n",
      "54 RF Pearson correlation: 0.7543356122161868 => 0.7542321953437321\n",
      "55 RF Pearson correlation: 0.7544378485019949 => 0.7542358677215582\n",
      "56 RF Pearson correlation: 0.7519590918125703 => 0.7541959242845584\n",
      "57 RF Pearson correlation: 0.7546392485801202 => 0.7542035678068957\n",
      "58 RF Pearson correlation: 0.7544561085770942 => 0.7542078481589329\n",
      "59 RF Pearson correlation: 0.7554906120718019 => 0.7542292275574808\n",
      "60 RF Pearson correlation: 0.7542217115667849 => 0.7542291043445185\n",
      "61 RF Pearson correlation: 0.7529185999250856 => 0.7542079671764632\n",
      "62 RF Pearson correlation: 0.7538936099018412 => 0.7542029773784534\n",
      "63 RF Pearson correlation: 0.7550507944985746 => 0.7542162245209553\n",
      "64 RF Pearson correlation: 0.7549769781138342 => 0.7542279284223842\n",
      "65 RF Pearson correlation: 0.7545121890077939 => 0.7542322354009509\n",
      "66 RF Pearson correlation: 0.7545646549138022 => 0.7542371968862173\n",
      "67 RF Pearson correlation: 0.7538635993432894 => 0.7542317028047036\n",
      "68 RF Pearson correlation: 0.7515672693903568 => 0.7541930878276841\n",
      "69 RF Pearson correlation: 0.7535132515893901 => 0.7541833758814228\n",
      "70 RF Pearson correlation: 0.7530969355683366 => 0.7541680739051821\n",
      "71 RF Pearson correlation: 0.7532103187501331 => 0.7541547717502509\n",
      "72 RF Pearson correlation: 0.7549385743064019 => 0.7541655087715681\n",
      "73 RF Pearson correlation: 0.7531493944570646 => 0.7541517774970476\n",
      "74 RF Pearson correlation: 0.7543366202000082 => 0.7541542420664205\n",
      "75 RF Pearson correlation: 0.754990476166654 => 0.7541652451466867\n",
      "76 RF Pearson correlation: 0.7559652107480271 => 0.7541886213233275\n",
      "77 RF Pearson correlation: 0.7524325771155238 => 0.754166107936048\n",
      "78 RF Pearson correlation: 0.7542660861373978 => 0.7541673734829005\n",
      "79 RF Pearson correlation: 0.7558214466869331 => 0.7541880493979509\n",
      "80 RF Pearson correlation: 0.7556395459108516 => 0.7542059691079867\n",
      "81 RF Pearson correlation: 0.7551173389798643 => 0.7542170833747169\n",
      "82 RF Pearson correlation: 0.7565885583554299 => 0.7542456553624364\n",
      "83 RF Pearson correlation: 0.7549022771291647 => 0.7542534722882307\n",
      "84 RF Pearson correlation: 0.7529213164279149 => 0.7542377998663445\n",
      "85 RF Pearson correlation: 0.7546077781549908 => 0.7542421019394684\n",
      "86 RF Pearson correlation: 0.7550938517406323 => 0.754251892167068\n",
      "87 RF Pearson correlation: 0.7544065130682256 => 0.7542536492227631\n",
      "88 RF Pearson correlation: 0.7531046443097581 => 0.7542407390552013\n",
      "89 RF Pearson correlation: 0.7552805738041676 => 0.7542522927746342\n",
      "90 RF Pearson correlation: 0.7544861444097363 => 0.7542548625728223\n",
      "91 RF Pearson correlation: 0.7535843016790222 => 0.7542475738674549\n",
      "92 RF Pearson correlation: 0.7539362964926963 => 0.7542442267989091\n",
      "93 RF Pearson correlation: 0.7559618745274572 => 0.7542624996470851\n",
      "94 RF Pearson correlation: 0.7530654504675593 => 0.7542498991294059\n",
      "95 RF Pearson correlation: 0.7536417595935588 => 0.7542435643425742\n",
      "96 RF Pearson correlation: 0.7548498396603274 => 0.754249814603582\n",
      "97 RF Pearson correlation: 0.7548674010886518 => 0.7542561165064907\n",
      "98 RF Pearson correlation: 0.7544354606751307 => 0.7542579280637497\n",
      "99 RF Pearson correlation: 0.753404496326399 => 0.7542493937463761\n",
      "Mean RF Pearson correlation: 0.7542493937463761\n"
     ]
    }
   ],
   "source": [
    "N_ITERS = 100\n",
    "rf_p = 0\n",
    "\n",
    "for i in range(N_ITERS):\n",
    "    # print('Iteration:', i)\n",
    "    # hist, model = train_NN(df, features, 'gs')\n",
    "    # df_test['predicted'] = model.predict(df_test[features])\n",
    "    # corr = pearsonr(df_test['gs'], df_test['predicted'])[0]\n",
    "    # print('NN Pearson correlation:', corr)\n",
    "    # nn_p += corr\n",
    "\n",
    "    model = train_single_RF(df_train, features, 'gs', params)\n",
    "    df_test['predicted'] = model.predict(df_test[features])\n",
    "    corr = pearsonr(df_test['gs'], df_test['predicted'])[0]\n",
    "    rf_p += corr\n",
    "    print(i, 'RF Pearson correlation:', corr, \"=>\", rf_p/(i+1))\n",
    "\n",
    "# print('NN Pearson correlation:', nn_p/N_ITERS)\n",
    "print('Mean RF Pearson correlation:', rf_p/N_ITERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare correlation based on file and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All SMTeuroparl 0.5323692675481492\n",
      "All SMTeuroparl 0.6311187599991405 (with surprise files)\n",
      "Synsets SMTeuroparl 0.48866057960531\n",
      "Synsets SMTeuroparl 0.5603342595419987 (with surprise files)\n",
      "Lemmas SMTeuroparl 0.48391137589586364\n",
      "Lemmas SMTeuroparl 0.58081638594915 (with surprise files)\n",
      "PoS (Syntactical) SMTeuroparl 0.2788364964179575\n",
      "PoS (Syntactical) SMTeuroparl 0.3112440728739055 (with surprise files)\n",
      "Lexical SMTeuroparl 0.5328963938746243\n",
      "Lexical SMTeuroparl 0.6266142158276385 (with surprise files)\n",
      "\n",
      "All MSRvid 0.8613777484336452\n",
      "All MSRvid 0.7915211726047531 (with surprise files)\n",
      "Synsets MSRvid 0.8324145109864043\n",
      "Synsets MSRvid 0.7352174235135138 (with surprise files)\n",
      "Lemmas MSRvid 0.6575259648502403\n",
      "Lemmas MSRvid 0.6414286178310427 (with surprise files)\n",
      "PoS (Syntactical) MSRvid 0.20795505045582274\n",
      "PoS (Syntactical) MSRvid 0.3528823063515935 (with surprise files)\n",
      "Lexical MSRvid 0.8615321031746392\n",
      "Lexical MSRvid 0.7774928297940386 (with surprise files)\n",
      "\n",
      "All MSRpar 0.59860392673409\n",
      "All MSRpar 0.5243800065566436 (with surprise files)\n",
      "Synsets MSRpar 0.4047525941241936\n",
      "Synsets MSRpar 0.35580425070606075 (with surprise files)\n",
      "Lemmas MSRpar 0.5537411634025081\n",
      "Lemmas MSRpar 0.5411226978763012 (with surprise files)\n",
      "PoS (Syntactical) MSRpar 0.3338367090736347\n",
      "PoS (Syntactical) MSRpar 0.025163526953436783 (with surprise files)\n",
      "Lexical MSRpar 0.5910819765065638\n",
      "Lexical MSRpar 0.539909094961663 (with surprise files)\n",
      "\n",
      "All All 0.8041477009785737\n",
      "All All 0.7534157915313975 (with surprise files)\n",
      "Synsets All 0.7653470999884919\n",
      "Synsets All 0.7087819265397867 (with surprise files)\n",
      "Lemmas All 0.6937530449417816\n",
      "Lemmas All 0.6420980675025608 (with surprise files)\n",
      "PoS (Syntactical) All 0.47796946976529026\n",
      "PoS (Syntactical) All 0.4536594552560736 (with surprise files)\n",
      "Lexical All 0.8035227451686744\n",
      "Lexical All 0.74595629479457 (with surprise files)\n"
     ]
    }
   ],
   "source": [
    "all_features = [\n",
    "            's1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_nouns', 's1_n_adjectives', 's1_n_adverbs', \n",
    "            's2_n_words', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_nouns', 's2_n_adjectives', 's2_n_adverbs', \n",
    "            'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', \n",
    "            \n",
    "            'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "\n",
    "            'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity',\n",
    "            'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity',\n",
    "            'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity',\n",
    "            'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity',\n",
    "            'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity',\n",
    "\n",
    "            'lemma_diversity', 'shared_lemmas_ratio', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient',\n",
    "            'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity'\n",
    "            ]\n",
    "word_features = [\n",
    "            's1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_nouns', 's1_n_adjectives', 's1_n_adverbs', \n",
    "            's2_n_words', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_nouns', 's2_n_adjectives', 's2_n_adverbs', \n",
    "            'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', \n",
    "            ]\n",
    "synset_features = [\n",
    "            'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "            ]\n",
    "lemma_features = [\n",
    "            'lemma_diversity', 'shared_lemmas_ratio', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient',\n",
    "            'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity'\n",
    "            ]\n",
    "lexical_features = [\n",
    "            'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "\n",
    "            'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity',\n",
    "            'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity',\n",
    "            'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity',\n",
    "            'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity',\n",
    "            'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity',\n",
    "\n",
    "            'lemma_diversity', 'shared_lemmas_ratio', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient',\n",
    "            'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity'\n",
    "]\n",
    "features_sets = [('All', all_features), ('Synsets', synset_features), ('Lemmas', lemma_features), ('PoS (Syntactical)', word_features), ('Lexical', lexical_features), ]\n",
    "files_sets = [\n",
    "    ('SMTeuroparl', ['SMTeuroparl'], ['SMTeuroparl', 'surprise.OnWN', 'surprise.SMTnews']),\n",
    "    ('MSRvid', ['MSRvid'], ['MSRvid', 'surprise.OnWN', 'surprise.SMTnews']),\n",
    "    ('MSRpar', ['MSRpar'], ['MSRpar', 'surprise.OnWN', 'surprise.SMTnews']),\n",
    "    ('All', ['SMTeuroparl', 'MSRvid', 'MSRpar'], ['SMTeuroparl', 'MSRvid', 'MSRpar', 'surprise.OnWN', 'surprise.SMTnews'])\n",
    "]\n",
    "\n",
    "N_ITERS = 10\n",
    "# Train a Random Forest for each feature set and each file set\n",
    "for t_name, tr_set, vl_set in files_sets:\n",
    "    print()\n",
    "    for f_name, f_set in features_sets:\n",
    "        train = df_train[df_train['file'].isin(tr_set)]\n",
    "        \n",
    "        test = df_test[df_test['file'].isin(tr_set)]\n",
    "        corr = 0\n",
    "        for i in range(N_ITERS):\n",
    "            model = train_single_RF(train, f_set, 'gs', params)\n",
    "            test.loc[:, 'predicted'] = model.predict(test[f_set])\n",
    "            corr += pearsonr(test['gs'], test['predicted'])[0]\n",
    "        print(f_name, t_name, corr / N_ITERS)\n",
    "\n",
    "        test = df_test[df_test['file'].isin(vl_set)]\n",
    "        corr = 0\n",
    "        for i in range(N_ITERS):\n",
    "            model = train_single_RF(train, f_set, 'gs', params)\n",
    "            test.loc[:, 'predicted'] = model.predict(test[f_set])\n",
    "            corr += pearsonr(test['gs'], test['predicted'])[0]\n",
    "        print(f_name, t_name, corr / N_ITERS, '(with surprise files)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it possible to predict separating by source file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in predicting file: 0.8249106687085248\n",
      "[2 2 2 ... 0 0 0]\n",
      "Number of rows in partition  0 : 750\n",
      "One model\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Number of rows in partition  1 : 750\n",
      "One model\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best Hyperparameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Number of rows in partition  2 : 734\n",
      "One model\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best Hyperparameters: {'bootstrap': True, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Predicted len:  1454\n",
      "Predicted len:  1354\n",
      "Predicted len:  300\n",
      "Final correlation:  0.7283554075731684\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Get a filtered test set\n",
    "filt_df_test = df_test[df_test['file'].isin(all_train_files)].copy()\n",
    "\n",
    "# Encode the file column to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_train['file_encoded'] = label_encoder.fit_transform(df_train['file'])\n",
    "filt_df_test.loc[:, 'file_encoded'] = label_encoder.transform(filt_df_test['file'])\n",
    "\n",
    "# On the train set, do a categorical encoding for the file column\n",
    "y_train = to_categorical(df_train['file_encoded'], num_classes=len(all_train_files))\n",
    "# Filter the test dataset and do the categorical encoding\n",
    "filt_y_test = to_categorical(filt_df_test['file_encoded'], num_classes=len(all_train_files))\n",
    "\n",
    "# Create a random forest classification model from df_train to y_train\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(df_train[all_features], y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(filt_df_test[all_features])\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = np.mean(np.argmax(filt_y_test, axis=1) == np.argmax(y_pred, axis=1))\n",
    "print('Accuracy in predicting file:', accuracy)\n",
    "\n",
    "# In the global train_set, assign the predicted file classç\n",
    "print(np.argmax(clf.predict(df_train[all_features]), axis=1))\n",
    "df_train['pred_file'] = np.argmax(clf.predict(df_train[all_features]), axis=1)\n",
    "\n",
    "# Train a regression random forest for each partition of df_train based on pred_file\n",
    "partitioned_models = []\n",
    "for file_class in range(len(all_train_files)):\n",
    "    partition = df_train[df_train['pred_file']== file_class]\n",
    "    print(\"Number of rows in partition \", file_class, \":\", partition.shape[0])\n",
    "    if not partition.empty:\n",
    "        print(\"One model\")\n",
    "        model, params = train_RF(partition, all_features, 'gs')\n",
    "        # model = RandomForestRegressor(**params)\n",
    "        # model.fit(partition[all_features], partition['gs'])\n",
    "        partitioned_models.append(model)\n",
    "\n",
    "# For each row in df_test, predict the file class and use the corresponding model to predict the predicted_gs\n",
    "# Predict the file class for each row in df_test\n",
    "df_test['pred_file'] = np.argmax(clf.predict(df_test[all_features]), axis=1)\n",
    "\n",
    "# For each partition of df_test based on pred_file, use the corresponding model to predict the gs\n",
    "for fcls in range(len(all_train_files)):\n",
    "    pred_gs = partitioned_models[fcls].predict(df_test[df_test['pred_file'] == fcls][all_features])\n",
    "    print(\"Predicted len: \", len(pred_gs))\n",
    "    df_test.loc[df_test['pred_file'] == fcls, 'gs_predicted'] = pred_gs\n",
    "\n",
    "# Compute the pearson correlation\n",
    "final_corr = pearsonr(df_test['gs'], df_test['gs_predicted'])[0]\n",
    "print(\"Final correlation: \", final_corr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
