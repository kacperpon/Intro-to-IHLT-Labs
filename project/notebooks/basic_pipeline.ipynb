{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHLT Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kacpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Kacpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kacpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Kacpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kacpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\Kacpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from features.statistical_features import FeatureExtractor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../data/train/01_raw/'\n",
    "TRAIN_GS_PATH = '../data/train/scores/'\n",
    "TEST_PATH = '../data/test/01_raw/'\n",
    "TEST_GS_PATH = '../data/test/scores/'\n",
    "TRAIN_SAVE_PATH = '../data/train/02_preprocessed/preprocessed_train_data.csv'\n",
    "TEST_SAVE_PATH = '../data/test/02_preprocessed/preprocessed_test_data.csv'\n",
    "PREDICTED_SAVE_PATH = '../data/test/03_predicted/'\n",
    "\n",
    "def load_data(path_f, path_gs, files):\n",
    "    # Read first file\n",
    "    dt = pd.read_csv(path_f + 'STS.input.' + files[0] + '.txt', sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['s1', 's2'])\n",
    "    dt['gs'] = pd.read_csv(path_gs + 'STS.gs.' + files[0] + '.txt', sep='\\t', header=None, names=['gs'])\n",
    "    dt['file']=files[0]\n",
    "    # Concatenate the rest of files\n",
    "    for f in files[1:]:\n",
    "        dt2 = pd.read_csv(path_f + 'STS.input.' + f + '.txt', sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['s1', 's2'])\n",
    "        dt2['gs'] = pd.read_csv(path_gs + 'STS.gs.' + f + '.txt', sep='\\t', header=None, names=['gs'])\n",
    "        dt2['file']=f\n",
    "        dt = pd.concat([dt, dt2], ignore_index=True)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def create_model(input_dim, learning_rate=0.001, neurons=10, hidden_layers=2):\n",
    "    \"\"\"\n",
    "    Function to create a Keras model with specified hyperparameters.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=input_dim, activation='relu'))  # Input layer\n",
    "    for _ in range(hidden_layers):  # Add hidden layers\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def train_NN(df, input, output):\n",
    "    \"\"\"\n",
    "    Train a neural network with grid search to find the best hyperparameters.\n",
    "    \"\"\"\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    # Wrap Keras model for compatibility with GridSearchCV\n",
    "    model = KerasRegressor(\n",
    "        model=create_model,\n",
    "        model__input_dim=input_dim,  \n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # TODO: Run this grid with beefier hardware, it's too slow for me to test\n",
    "    # Define hyperparameter grid\n",
    "    # param_grid = {\n",
    "    #     'batch_size': [16, 32, 64],\n",
    "    #     'epochs': [50, 100],\n",
    "    #     'learning_rate': [0.001, 0.01],\n",
    "    #     'neurons': [5, 10, 20],\n",
    "    #     'hidden_layers': [2, 3, 4]\n",
    "    # }\n",
    "\n",
    "    param_grid = {\n",
    "        \"model__neurons\": [5, 10],\n",
    "        \"model__hidden_layers\": [2],\n",
    "        \"model__learning_rate\": [0.001],\n",
    "        \"batch_size\": [16, 32],\n",
    "        \"epochs\": [50, 100],\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=3,\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# TODO: 3 combinations failing, investigate why + how to fix\n",
    "def train_MLP(df, input, output):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "\n",
    "    # Define the base model\n",
    "    model = MLPRegressor(max_iter=1000, early_stopping=True, validation_fraction=0.1, verbose=False)\n",
    "\n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(10,), (50,), (100,)],  # Vary number and size of layers\n",
    "        'activation': ['relu', 'tanh'],                                       # Activation functions\n",
    "        'solver': ['adam', 'sgd'],                                            # Optimization solvers\n",
    "        'alpha': [0.0001, 0.001, 0.01],                                       # Regularization strength\n",
    "        'learning_rate': ['constant', 'adaptive'],                            # Learning rate schedules\n",
    "    }\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',  # Use MSE as the evaluation metric\n",
    "        cv=3,                              # 3-fold cross-validation\n",
    "        verbose=2,                         # Display progress\n",
    "        n_jobs=-1,                          # Use all available cores\n",
    "    )\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    results = grid_search.cv_results_\n",
    "    for mean_score, params in zip(results['mean_test_score'], results['params']):\n",
    "        if np.isnan(mean_score):\n",
    "            print(\"Failed combination:\", params)\n",
    "\n",
    "    # Best model from grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def train_RF(df, input, output):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "\n",
    "    # Define the model\n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    # TODO: Run this grid with beefier hardware, it's too slow for me to test\n",
    "    # Define the grid of hyperparameters to search\n",
    "    # param_grid = {\n",
    "    #     'n_estimators': [100, 200, 300],  \n",
    "    #     'max_depth': [None, 10, 20, 30], \n",
    "    #     'min_samples_split': [2, 5, 10], \n",
    "    #     'min_samples_leaf': [1, 2, 4],   \n",
    "    #     'max_features': ['sqrt', 'log2'],\n",
    "    #     'bootstrap': [True, False],      \n",
    "    # }\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],  \n",
    "        'max_depth': [None, 10], \n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],  \n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'bootstrap': [True, False],     \n",
    "    }\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',  \n",
    "        cv=5,                              \n",
    "        verbose=2,                         \n",
    "        n_jobs=-1                          \n",
    "    )\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Best model from grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# TODO: Optimise this - too slow\n",
    "def train_SVR(df, input, output):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "    \n",
    "    model = SVR()\n",
    "    \n",
    "    param_grid = {\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'degree': [2, 3, 4] \n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=10)\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best params:\", best_params)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(history.history['mae'])\n",
    "    plt.plot(history.history['val_mae'])\n",
    "    plt.title('model mae')\n",
    "    plt.ylabel('mae')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pipeline\n",
    "\n",
    "### Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data\n",
      "Adding POS based features...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m df_train \u001b[38;5;241m=\u001b[39m load_data(TRAIN_PATH, TRAIN_GS_PATH, all_train_files)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Add features to the train data\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43madd_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Save df to a file\u001b[39;00m\n\u001b[0;32m     18\u001b[0m df_train\u001b[38;5;241m.\u001b[39mto_csv(TRAIN_SAVE_PATH, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[82], line 5\u001b[0m, in \u001b[0;36madd_features\u001b[1;34m(dt)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_features\u001b[39m(dt):\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_POS_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     feature_extractor\u001b[38;5;241m.\u001b[39madd_synset_statistics_ext(dt)\n\u001b[0;32m      7\u001b[0m     feature_extractor\u001b[38;5;241m.\u001b[39madd_lemma_statistics(dt)\n",
      "File \u001b[1;32mc:\\Users\\Kacpe\\Desktop\\College Work\\Repos\\Intro to HLT\\Work\\Labs\\project\\notebooks\\features\\statistical_features.py:51\u001b[0m, in \u001b[0;36mFeatureExtractor.add_POS_statistics\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Number of verbs\u001b[39;00m\n\u001b[0;32m     50\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms1_n_verbs_tot\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m pos[i][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m word[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mVERBS])\n\u001b[1;32m---> 51\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms2_n_verbs_tot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m pos[i][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m word[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mVERBS])\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Verbs in present\u001b[39;00m\n\u001b[0;32m     53\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms1_n_verbs_pres\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m pos[i][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m word[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mVERBS_PRESENT])\n",
      "File \u001b[1;32mc:\\Users\\Kacpe\\Desktop\\College Work\\Repos\\Intro to HLT\\Work\\Labs\\project\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:907\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    905\u001b[0m     maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m    906\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m--> 907\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_setitem_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n",
      "File \u001b[1;32mc:\\Users\\Kacpe\\Desktop\\College Work\\Repos\\Intro to HLT\\Work\\Labs\\project\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:774\u001b[0m, in \u001b[0;36m_LocationIndexer._get_setitem_indexer\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m    773\u001b[0m         \u001b[38;5;66;03m# suppress \"Too many indexers\"\u001b[39;00m\n\u001b[1;32m--> 774\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mrange\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#45479 test_loc_setitem_range_key\u001b[39;00m\n\u001b[0;32m    778\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n",
      "File \u001b[1;32mc:\\Users\\Kacpe\\Desktop\\College Work\\Repos\\Intro to HLT\\Work\\Labs\\project\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:989\u001b[0m, in \u001b[0;36m_LocationIndexer._convert_tuple\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;66;03m# Note: we assume _tupleize_axis_indexer has been called, if necessary.\u001b[39;00m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key_length(key)\n\u001b[1;32m--> 989\u001b[0m     keyidx \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_to_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(key)]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(keyidx)\n",
      "File \u001b[1;32mc:\\Users\\Kacpe\\Desktop\\College Work\\Repos\\Intro to HLT\\Work\\Labs\\project\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1452\u001b[0m, in \u001b[0;36m_LocIndexer._convert_to_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1447\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1448\u001b[0m         \u001b[38;5;66;03m# DatetimeIndex overrides Index.slice_indexer and may\u001b[39;00m\n\u001b[0;32m   1449\u001b[0m         \u001b[38;5;66;03m#  return a DatetimeIndex instead of a slice object.\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mtake(indexer, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m-> 1452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_to_indexer\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, axis: AxisInt):\n\u001b[0;32m   1453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1454\u001b[0m \u001b[38;5;124;03m    Convert indexing key into something we can use to do actual fancy\u001b[39;00m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;124;03m    indexing on a ndarray.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1465\u001b[0m \u001b[38;5;124;03m    - No, prefer label-based indexing\u001b[39;00m\n\u001b[0;32m   1466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1467\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Create the desired features\n",
    "def add_features(dt):\n",
    "    feature_extractor.add_POS_statistics(dt)\n",
    "    feature_extractor.add_synset_statistics_ext(dt)\n",
    "    feature_extractor.add_lemma_statistics(dt)\n",
    "\n",
    "# Load train data\n",
    "print('Loading train data')\n",
    "all_train_files = ['SMTeuroparl', 'MSRvid', 'MSRpar']\n",
    "df_train = load_data(TRAIN_PATH, TRAIN_GS_PATH, all_train_files)\n",
    "\n",
    "# Add features to the train data\n",
    "add_features(df_train)\n",
    "\n",
    "# Save df to a file\n",
    "df_train.to_csv(TRAIN_SAVE_PATH, index=False)\n",
    "\n",
    "# Load test data\n",
    "print('Loading test data')\n",
    "all_test_files = ['SMTeuroparl', 'MSRvid', 'MSRpar', 'surprise.OnWN', 'surprise.SMTnews']\n",
    "df_test = load_data(TEST_PATH, TEST_GS_PATH, all_test_files)\n",
    "\n",
    "# Add the features to the test data\n",
    "add_features(df_test)\n",
    "\n",
    "# Save df_test to a file\n",
    "df_test.to_csv(TEST_SAVE_PATH, index=False)\n",
    "\n",
    "print('Train and test datasets ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor()\n",
    "#feature_extractor.add_Word_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframes from files\n",
    "df_train = pd.read_csv(TRAIN_SAVE_PATH)\n",
    "df_test = pd.read_csv(TEST_SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "Best Hyperparameters: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Features to use in the model\n",
    "features = [\n",
    "            's1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_nouns', 's1_n_adjectives', 's1_n_adverbs', \n",
    "            's2_n_words', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_nouns', 's2_n_adjectives', 's2_n_adverbs', \n",
    "            'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', \n",
    "            \n",
    "            'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "\n",
    "            'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity',\n",
    "            'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity',\n",
    "            'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity',\n",
    "            'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity',\n",
    "            'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity',\n",
    "\n",
    "            'lemma_diversity', 'shared_lemmas_ratio', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient',\n",
    "            'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity'\n",
    "            ]\n",
    "\n",
    "\n",
    "# Train a NN\n",
    "# model = train_NN(df_train, features, 'gs')\n",
    "# plot_history(hist)\n",
    "\n",
    "# Train a MLP\n",
    "best_mlp_model = train_MLP(df_train, features, 'gs')\n",
    "\n",
    "# Train a Random Forest\n",
    "# best_rf_model = train_random_forest_with_grid_search(df_train, features, 'gs')\n",
    "\n",
    "# Train a Support vector regression\n",
    "# model = train_svr(df, features, 'gs')\n",
    "\n",
    "# Find best svr\n",
    "# model = train_svr_with_grid_search(df, features, 'gs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute correlation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation: 0.7199680422975143\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Fill column of the dataset with the predictions of the model\n",
    "# df_test['predicted'] = best_rf_model.predict(df_test[features])\n",
    "#df_test['predicted'] = best_mlp_model.predict(df_test[features])\n",
    "df_test['predicted'] = best_mlp_model.predict(df_test[features])\n",
    "\n",
    "# Compute the Pearson correlation between the predictions and the gold standard\n",
    "corr = pearsonr(df_test['gs'], df_test['predicted'])[0]\n",
    "print('Pearson correlation:', corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kacpe\\AppData\\Local\\Temp\\ipykernel_22796\\2052023131.py:21: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df.applymap(clean_value)\n"
     ]
    }
   ],
   "source": [
    "# Save the predicted dataset\n",
    "# Add timestamp to the name of the file\n",
    "import re\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "df_test.to_csv(PREDICTED_SAVE_PATH + timestamp + '_predicted_test_data.csv', index=False)\n",
    "\n",
    "\n",
    "def clean_illegal_characters(df):\n",
    "    # Definir una expressió regular per trobar caràcters il·legals\n",
    "    illegal_characters_re = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "    \n",
    "    # Funció per netejar un valor individual\n",
    "    def clean_value(value):\n",
    "        if isinstance(value, str):\n",
    "            return illegal_characters_re.sub('', value)\n",
    "        return value\n",
    "    \n",
    "    # Aplicar la funció de neteja a cada cel·la del DataFrame\n",
    "    return df.applymap(clean_value)\n",
    "\n",
    "df_clean = clean_illegal_characters(df_test)\n",
    "df_clean.to_excel(PREDICTED_SAVE_PATH + timestamp + '_predicted_test_data.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Pearson correlation: 0.7340141820239494\n",
      "RF Pearson correlation: 0.7363241823993593\n",
      "RF Pearson correlation: 0.7360199716898412\n",
      "RF Pearson correlation: 0.7324130149378948\n",
      "RF Pearson correlation: 0.7355748836441798\n",
      "RF Pearson correlation: 0.7348586281518488\n",
      "RF Pearson correlation: 0.7378853670909417\n",
      "RF Pearson correlation: 0.7354003696446679\n",
      "RF Pearson correlation: 0.7380812274067033\n",
      "RF Pearson correlation: 0.7330334611578138\n",
      "RF Pearson correlation: 0.73536052881472\n"
     ]
    }
   ],
   "source": [
    "N_ITERS = 10\n",
    "nn_p = 0\n",
    "rf_p = 0\n",
    "\n",
    "for i in range(N_ITERS):\n",
    "    # print('Iteration:', i)\n",
    "    # hist, model = train_NN(df, features, 'gs')\n",
    "    # df_test['predicted'] = model.predict(df_test[features])\n",
    "    # corr = pearsonr(df_test['gs'], df_test['predicted'])[0]\n",
    "    # print('NN Pearson correlation:', corr)\n",
    "    # nn_p += corr\n",
    "\n",
    "    model = train_random_forest(df_train, features, 'gs')\n",
    "    df_test['predicted'] = model.predict(df_test[features])\n",
    "    corr = pearsonr(df_test['gs'], df_test['predicted'])[0]\n",
    "    print('RF Pearson correlation:', corr)\n",
    "    rf_p += corr\n",
    "\n",
    "# print('NN Pearson correlation:', nn_p/N_ITERS)\n",
    "print('RF Pearson correlation:', rf_p/N_ITERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
