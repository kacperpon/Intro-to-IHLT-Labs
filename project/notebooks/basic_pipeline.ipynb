{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHLT Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kacpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Kacpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kacpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Kacpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kacpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\Kacpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet_ic.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "from utils.preprocessor import Preprocessor\n",
    "from features.dummy_features import add_dummy_features\n",
    "from features.statistical_features import add_POS_statistics, add_synset_statistics, add_synset_statistics_ext, add_lemma_statistics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../data/train/01_raw/'\n",
    "TRAIN_GS_PATH = '../data/train/scores/'\n",
    "TEST_PATH = '../data/test/01_raw/'\n",
    "TEST_GS_PATH = '../data/test/scores/'\n",
    "\n",
    "def load_data(path_f, path_gs, files):\n",
    "    # Read first file\n",
    "    dt = pd.read_csv(path_f + 'STS.input.' + files[0] + '.txt', sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['s1', 's2'])\n",
    "    dt['gs'] = pd.read_csv(path_gs + 'STS.gs.' + files[0] + '.txt', sep='\\t', header=None, names=['gs'])\n",
    "    # Concatenate the rest of files\n",
    "    for f in files[1:]:\n",
    "        dt2 = pd.read_csv(path_f + 'STS.input.' + f + '.txt', sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['s1', 's2'])\n",
    "        dt2['gs'] = pd.read_csv(path_gs + 'STS.gs.' + f + '.txt', sep='\\t', header=None, names=['gs'])\n",
    "        dt = pd.concat([dt, dt2], ignore_index=True)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN(df, input, output):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)\n",
    "    model = Sequential()\n",
    "    # model.add(Dense(10, input_dim=len(input), activation='relu'))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dense(5, activation='relu'))               \n",
    "    model.add(Dense(5, activation='relu'))               \n",
    "    model.add(Dense(1))                                   \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32, verbose=0)\n",
    "    return history, model\n",
    "\n",
    "def train_MLP(df, input, output):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=(5, 5, 5), \n",
    "        max_iter=500, \n",
    "        random_state=42, \n",
    "        early_stopping=True, \n",
    "        validation_fraction=0.1,  \n",
    "        verbose=True\n",
    "    )\n",
    "    history = model.fit(X, y)\n",
    "    return history, model\n",
    "\n",
    "def train_random_forest(df, input, output):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "\n",
    "    # Definim i entrenem el model Random Forest\n",
    "    model = RandomForestRegressor(n_estimators=50)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def train_svr(df, input, output, kernel='rbf', C=1.0, epsilon=0.1):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "    \n",
    "    model = SVR(kernel=kernel, C=C, epsilon=epsilon)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_svr_with_grid_search(df, input, output):\n",
    "    X = df[input]\n",
    "    y = df[output]\n",
    "    \n",
    "    model = SVR()\n",
    "    \n",
    "    param_grid = {\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'degree': [2, 3, 4] \n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=10)\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best params:\", best_params)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(history.history['mae'])\n",
    "    plt.plot(history.history['val_mae'])\n",
    "    plt.title('model mae')\n",
    "    plt.ylabel('mae')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pipeline\n",
    "\n",
    "### Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data\n",
      "Loading test data\n",
      "Train and test datasets ready\n"
     ]
    }
   ],
   "source": [
    "# Create the desired features\n",
    "def add_features(dt):\n",
    "    # add_POS_statistics(dt)\n",
    "    # add_synset_statistics_ext(dt)\n",
    "    add_lemma_statistics(dt)\n",
    "\n",
    "# Load train data\n",
    "print('Loading train data')\n",
    "all_train_files = ['SMTeuroparl', 'MSRvid', 'MSRpar']\n",
    "df = load_data(TRAIN_PATH, TRAIN_GS_PATH, all_train_files)\n",
    "# Add features to the train data\n",
    "add_features(df)\n",
    "# Print all columns of resulting df\n",
    "# print(df.columns)\n",
    "\n",
    "# Load test data\n",
    "print('Loading test data')\n",
    "all_test_files = ['SMTeuroparl', 'MSRvid', 'MSRpar', 'surprise.OnWN', 'surprise.SMTnews']\n",
    "df_test = load_data(TEST_PATH, TEST_GS_PATH, all_test_files)\n",
    "# Add the features to the test data\n",
    "add_features(df_test)\n",
    "\n",
    "print('Train and test datasets ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to use in the model\n",
    "features = [\n",
    "            # 's1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_nouns', 's1_n_adjectives', 's1_n_adverbs', \n",
    "            # # 's2_n_words', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_nouns', 's2_n_adjectives', 's2_n_adverbs', \n",
    "            # 'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', \n",
    "            \n",
    "            # 'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            # 'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            # 'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            # 'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            # 'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "\n",
    "            # 'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity',\n",
    "            # 'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity',\n",
    "            # 'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity',\n",
    "            # 'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity',\n",
    "            # 'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity',\n",
    "\n",
    "            'lemma_diversity', 'shared_lemmas_ratio', 'avg_lemma_similarity', 'max_lemma_similarity'\n",
    "            ]\n",
    "\n",
    "# Train a NN\n",
    "hist, model = train_NN(df, features, 'gs')\n",
    "# plot_history(hist)\n",
    "\n",
    "# Train a MLP\n",
    "# hist, model = train_MLP(df, features, 'gs')\n",
    "\n",
    "# Train a Random Forest\n",
    "# model = train_random_forest(df, features, 'gs')\n",
    "\n",
    "# Train a Support vector regression\n",
    "# model = train_svr(df, features, 'gs')\n",
    "\n",
    "# Find best svr\n",
    "# model = train_svr_with_grid_search(df, features, 'gs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute correlation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Pearson correlation: 0.5462109997596549\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Fill column of the dataset with the predictions of the model\n",
    "df_test['predicted'] = model.predict(df_test[features])\n",
    "\n",
    "# Compute the Pearson correlation between the predictions and the gold standard\n",
    "corr = pearsonr(df_test['gs'], df_test['predicted'])[0]\n",
    "print('Pearson correlation:', corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Pearson correlation: 0.6981405928635337\n",
      "RF Pearson correlation: 0.7016522605995392\n",
      "RF Pearson correlation: 0.6937648052383595\n",
      "RF Pearson correlation: 0.6915285461003057\n",
      "RF Pearson correlation: 0.6939374465510104\n",
      "RF Pearson correlation: 0.6968994286868317\n",
      "RF Pearson correlation: 0.7035726529316195\n",
      "RF Pearson correlation: 0.7018517155577366\n",
      "RF Pearson correlation: 0.6952235142668917\n",
      "RF Pearson correlation: 0.6967814757949129\n",
      "NN Pearson correlation: 0.0\n",
      "RF Pearson correlation: 0.6973352438590741\n"
     ]
    }
   ],
   "source": [
    "N_ITERS = 10\n",
    "nn_p = 0\n",
    "rf_p = 0\n",
    "\n",
    "for i in range(N_ITERS):\n",
    "    # print('Iteration:', i)\n",
    "    # hist, model = train_NN(df, features, 'gs')\n",
    "    # df_test['predicted'] = model.predict(df_test[features])\n",
    "    # corr = pearsonr(df_test['gs'], df_test['predicted'])[0]\n",
    "    # print('NN Pearson correlation:', corr)\n",
    "    # nn_p += corr\n",
    "\n",
    "    model = train_random_forest(df, features, 'gs')\n",
    "    df_test['predicted'] = model.predict(df_test[features])\n",
    "    corr = pearsonr(df_test['gs'], df_test['predicted'])[0]\n",
    "    print('RF Pearson correlation:', corr)\n",
    "    rf_p += corr\n",
    "\n",
    "# print('NN Pearson correlation:', nn_p/N_ITERS)\n",
    "print('RF Pearson correlation:', rf_p/N_ITERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
