{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Textual Similarity Project\n",
    "\n",
    "**Authors**\n",
    "- Kacper Poniatowski\n",
    "- Pau Blanco\n",
    "\n",
    "TODO: Include brief outline of project\n",
    "- Context\n",
    "- What were trying to achieve\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Reqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Force auto-reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from utils import load_data, evaluate_rf_model, drop_highly_correlated_features, update_results_csv, generate_plots_from_metrics\n",
    "from models import ModelTrainer\n",
    "from feature_extraction import FeatureExtractor\n",
    "from utils import save_predictions\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constant paths used throughout notebook\n",
    "TRAIN_PATH = '../data/train/01_raw/'\n",
    "TRAIN_GS_PATH = '../data/train/scores/'\n",
    "TEST_PATH = '../data/test/01_raw/'\n",
    "TEST_GS_PATH = '../data/test/scores/'\n",
    "TRAIN_SAVE_PATH = '../data/train/02_preprocessed/preprocessed_train_data.csv'\n",
    "TEST_SAVE_PATH = '../data/test/02_preprocessed/preprocessed_test_data.csv'\n",
    "PREDICTED_SAVE_PATH = '../data/test/03_predicted/'\n",
    "RESULTS_SAVE_PATH = \"project/test/03_predicted/results.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading train data\n",
      "\n",
      " Loading test data\n",
      "\n",
      " Train and test datasets loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "print('\\n Loading train data')\n",
    "all_train_files = ['SMTeuroparl', 'MSRvid', 'MSRpar']\n",
    "df_train = load_data(TRAIN_PATH, TRAIN_GS_PATH, all_train_files)\n",
    "\n",
    "# Load test data\n",
    "print('\\n Loading test data')\n",
    "all_test_files = ['SMTeuroparl', 'MSRvid', 'MSRpar', 'surprise.OnWN', 'surprise.SMTnews']\n",
    "df_test = load_data(TEST_PATH, TEST_GS_PATH, all_test_files)\n",
    "\n",
    "print('\\n Train and test datasets loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding POS based features...\n",
      "Adding synset-based features...\n",
      "Processed 2234 of 2234 rows (100%)      \n",
      "Adding lemma based features...\n",
      "Adding POS based features...\n",
      "Adding synset-based features...\n",
      "Processed 3108 of 3108 rows (100%)      \n",
      "Adding lemma based features...\n",
      "\n",
      " Features added to datasets successfully\n",
      "\n",
      " Train and test datasets saved as .csv files successfully\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Extract the desired features\n",
    "def add_features(dt):\n",
    "    feature_extractor.add_POS_statistics(dt)\n",
    "    feature_extractor.add_synset_statistics(dt)\n",
    "    feature_extractor.add_lemma_statistics(dt)\n",
    "\n",
    "# Add features to the training and test data\n",
    "add_features(df_train)\n",
    "add_features(df_test)\n",
    "\n",
    "print('\\n Features added to datasets successfully')\n",
    "\n",
    "# Save df_train and df_test to respective files - this is done to avoid re-running the\n",
    "# feature extraction process each time as synset extraction is computationally expensive\n",
    "df_test.to_csv(TEST_SAVE_PATH, index=False)\n",
    "df_train.to_csv(TRAIN_SAVE_PATH, index=False)\n",
    "\n",
    "print('\\n Train and test datasets saved as .csv files successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Score Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Existing Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train and test datasets loaded from .csv files successfully\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_SAVE_PATH is None or TEST_SAVE_PATH is None:\n",
    "    raise ValueError('TRAIN_SAVE_PATH and TEST_SAVE_PATH must be defined')\n",
    "\n",
    "if not os.path.exists(TRAIN_SAVE_PATH):\n",
    "    raise FileNotFoundError(f'The file {TRAIN_SAVE_PATH} does not exist')\n",
    "\n",
    "if not os.path.exists(TEST_SAVE_PATH):\n",
    "    raise FileNotFoundError(f'The file {TEST_SAVE_PATH} does not exist')\n",
    "\n",
    "original_df_train = pd.read_csv(TRAIN_SAVE_PATH)\n",
    "original_df_test = pd.read_csv(TEST_SAVE_PATH)\n",
    "\n",
    "print('\\n Train and test datasets loaded from .csv files successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           s1_n_words  s2_n_words  s1_n_verbs_tot  \\\n",
      "s1_n_words                   1.000000    0.918735        0.758300   \n",
      "s2_n_words                   0.918735    1.000000        0.673713   \n",
      "s1_n_verbs_tot               0.758300    0.673713        1.000000   \n",
      "s2_n_verbs_tot               0.590479    0.698453        0.740191   \n",
      "s1_n_verbs_pres              0.246101    0.191457        0.571911   \n",
      "...                               ...         ...             ...   \n",
      "lemma_lcs_length             0.814533    0.793255        0.545660   \n",
      "lemma_edit_distance          0.781976    0.809274        0.570023   \n",
      "proportion_s1_in_s2          0.202598    0.231090        0.077001   \n",
      "proportion_s2_in_s1          0.298470    0.213312        0.158630   \n",
      "lemma_position_similarity    0.153036    0.110382        0.118022   \n",
      "\n",
      "                           s2_n_verbs_tot  s1_n_verbs_pres  s2_n_verbs_pres  \\\n",
      "s1_n_words                       0.590479         0.246101         0.113169   \n",
      "s2_n_words                       0.698453         0.191457         0.203622   \n",
      "s1_n_verbs_tot                   0.740191         0.571911         0.352796   \n",
      "s2_n_verbs_tot                   1.000000         0.359068         0.585477   \n",
      "s1_n_verbs_pres                  0.359068         1.000000         0.583186   \n",
      "...                                   ...              ...              ...   \n",
      "lemma_lcs_length                 0.470067         0.036863        -0.021604   \n",
      "lemma_edit_distance              0.542890         0.157074         0.102533   \n",
      "proportion_s1_in_s2              0.086514        -0.158674        -0.147527   \n",
      "proportion_s2_in_s1              0.070091        -0.122421        -0.165168   \n",
      "lemma_position_similarity        0.040985         0.016534        -0.049000   \n",
      "\n",
      "                           s1_n_verbs_past  s2_n_verbs_past  s1_n_nouns  \\\n",
      "s1_n_words                        0.541747         0.366501    0.879111   \n",
      "s2_n_words                        0.477406         0.426594    0.812494   \n",
      "s1_n_verbs_tot                    0.616441         0.373132    0.520241   \n",
      "s2_n_verbs_tot                    0.473997         0.555110    0.405833   \n",
      "s1_n_verbs_pres                  -0.088747        -0.161231    0.066217   \n",
      "...                                    ...              ...         ...   \n",
      "lemma_lcs_length                  0.519421         0.441736    0.812901   \n",
      "lemma_edit_distance               0.443265         0.373852    0.741351   \n",
      "proportion_s1_in_s2               0.210567         0.230728    0.230298   \n",
      "proportion_s2_in_s1               0.269901         0.213446    0.330817   \n",
      "lemma_position_similarity         0.090492         0.038153    0.128458   \n",
      "\n",
      "                           s2_n_nouns  ...  max_lemma_similarity  \\\n",
      "s1_n_words                   0.822095  ...              0.229858   \n",
      "s2_n_words                   0.884512  ...              0.150631   \n",
      "s1_n_verbs_tot               0.481373  ...              0.147050   \n",
      "s2_n_verbs_tot               0.451101  ...              0.017157   \n",
      "s1_n_verbs_pres              0.045344  ...             -0.036966   \n",
      "...                               ...  ...                   ...   \n",
      "lemma_lcs_length             0.808128  ...              0.301991   \n",
      "lemma_edit_distance          0.758161  ...              0.067852   \n",
      "proportion_s1_in_s2          0.283861  ...              0.543419   \n",
      "proportion_s2_in_s1          0.239628  ...              0.546156   \n",
      "lemma_position_similarity    0.109522  ...              0.828971   \n",
      "\n",
      "                           lemma_jackard_similarity  shared_lemma_count  \\\n",
      "s1_n_words                                 0.224797            0.823098   \n",
      "s2_n_words                                 0.204095            0.802170   \n",
      "s1_n_verbs_tot                             0.093844            0.546626   \n",
      "s2_n_verbs_tot                             0.066700            0.463084   \n",
      "s1_n_verbs_pres                           -0.138852            0.036443   \n",
      "...                                             ...                 ...   \n",
      "lemma_lcs_length                           0.552054            0.965594   \n",
      "lemma_edit_distance                       -0.085678            0.576657   \n",
      "proportion_s1_in_s2                        0.941852            0.575162   \n",
      "proportion_s2_in_s1                        0.936890            0.599770   \n",
      "lemma_position_similarity                  0.394989            0.214088   \n",
      "\n",
      "                           dice_coefficient  lemma_bigram_overlap  \\\n",
      "s1_n_words                         0.264632              0.169512   \n",
      "s2_n_words                         0.236947              0.153650   \n",
      "s1_n_verbs_tot                     0.125389              0.060910   \n",
      "s2_n_verbs_tot                     0.085188              0.051548   \n",
      "s1_n_verbs_pres                   -0.144292             -0.128608   \n",
      "...                                     ...                   ...   \n",
      "lemma_lcs_length                   0.582465              0.479089   \n",
      "lemma_edit_distance               -0.042950             -0.110112   \n",
      "proportion_s1_in_s2                0.962305              0.731835   \n",
      "proportion_s2_in_s1                0.959543              0.738100   \n",
      "lemma_position_similarity          0.499750              0.201317   \n",
      "\n",
      "                           lemma_lcs_length  lemma_edit_distance  \\\n",
      "s1_n_words                         0.814533             0.781976   \n",
      "s2_n_words                         0.793255             0.809274   \n",
      "s1_n_verbs_tot                     0.545660             0.570023   \n",
      "s2_n_verbs_tot                     0.470067             0.542890   \n",
      "s1_n_verbs_pres                    0.036863             0.157074   \n",
      "...                                     ...                  ...   \n",
      "lemma_lcs_length                   1.000000             0.508607   \n",
      "lemma_edit_distance                0.508607             1.000000   \n",
      "proportion_s1_in_s2                0.542896            -0.061796   \n",
      "proportion_s2_in_s1                0.568452            -0.020979   \n",
      "lemma_position_similarity          0.263695            -0.094165   \n",
      "\n",
      "                           proportion_s1_in_s2  proportion_s2_in_s1  \\\n",
      "s1_n_words                            0.202598             0.298470   \n",
      "s2_n_words                            0.231090             0.213312   \n",
      "s1_n_verbs_tot                        0.077001             0.158630   \n",
      "s2_n_verbs_tot                        0.086514             0.070091   \n",
      "s1_n_verbs_pres                      -0.158674            -0.122421   \n",
      "...                                        ...                  ...   \n",
      "lemma_lcs_length                      0.542896             0.568452   \n",
      "lemma_edit_distance                  -0.061796            -0.020979   \n",
      "proportion_s1_in_s2                   1.000000             0.852209   \n",
      "proportion_s2_in_s1                   0.852209             1.000000   \n",
      "lemma_position_similarity             0.484480             0.473087   \n",
      "\n",
      "                           lemma_position_similarity  \n",
      "s1_n_words                                  0.153036  \n",
      "s2_n_words                                  0.110382  \n",
      "s1_n_verbs_tot                              0.118022  \n",
      "s2_n_verbs_tot                              0.040985  \n",
      "s1_n_verbs_pres                             0.016534  \n",
      "...                                              ...  \n",
      "lemma_lcs_length                            0.263695  \n",
      "lemma_edit_distance                        -0.094165  \n",
      "proportion_s1_in_s2                         0.484480  \n",
      "proportion_s2_in_s1                         0.473087  \n",
      "lemma_position_similarity                   1.000000  \n",
      "\n",
      "[79 rows x 79 columns]\n",
      "\n",
      " Total number of columns (features) in the correlation matrix: 79\n"
     ]
    }
   ],
   "source": [
    "# Drop first 4 columns to remove non-numerical columns\n",
    "df_train = original_df_train.drop(original_df_train.columns[:4], axis=1)\n",
    "df_test = original_df_test.drop(original_df_test.columns[:4], axis=1)\n",
    "\n",
    "correlation_matrix = df_train.corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Total number of columns in the correlation matrix\n",
    "total_cols = correlation_matrix.shape[1]\n",
    "print(f'\\n Total number of columns (features) in the correlation matrix: {total_cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find best correlation threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feaures: 79\n",
      "\n",
      "Dropping columns with correlation above 0.7\n",
      "Columns dropped: 41\n",
      "Remaining columns: 38\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.750920528636134\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7513897840948833\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\test_0.7\\2024-12-11_18-25-38_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\test_0.7\\2024-12-11_18-25-38_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\test_0.7\\2024-12-11_18-25-38_test_data.xlsx\n",
      "\n",
      "Dropping columns with correlation above 0.75\n",
      "Columns dropped: 36\n",
      "Remaining columns: 43\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7623454002064807\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7591629084486324\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\test_0.75\\2024-12-11_18-26-26_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\test_0.75\\2024-12-11_18-26-26_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\test_0.75\\2024-12-11_18-26-26_test_data.xlsx\n",
      "\n",
      "Dropping columns with correlation above 0.8\n",
      "Columns dropped: 32\n",
      "Remaining columns: 47\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7634948927164802\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.762263090528221\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\test_0.8\\2024-12-11_18-27-29_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\test_0.8\\2024-12-11_18-27-29_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\test_0.8\\2024-12-11_18-27-29_test_data.xlsx\n",
      "\n",
      "Dropping columns with correlation above 0.85\n",
      "Columns dropped: 26\n",
      "Remaining columns: 53\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7520333671236472\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7531644600918147\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\test_0.85\\2024-12-11_18-28-37_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\test_0.85\\2024-12-11_18-28-37_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\test_0.85\\2024-12-11_18-28-37_test_data.xlsx\n",
      "\n",
      "Dropping columns with correlation above 0.9\n",
      "Columns dropped: 14\n",
      "Remaining columns: 65\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7580475730311692\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7590360612800903\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\test_0.9\\2024-12-11_18-29-41_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\test_0.9\\2024-12-11_18-29-41_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\test_0.9\\2024-12-11_18-29-41_test_data.xlsx\n",
      "\n",
      "Dropping columns with correlation above 0.95\n",
      "Columns dropped: 10\n",
      "Remaining columns: 69\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7612824081096524\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7599192290067266\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\test_0.95\\2024-12-11_18-30-57_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\test_0.95\\2024-12-11_18-30-57_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\test_0.95\\2024-12-11_18-30-57_test_data.xlsx\n",
      "\n",
      "Dropping columns with correlation above 1\n",
      "Columns dropped: 0\n",
      "Remaining columns: 79\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7524110007850136\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7524362569046683\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\test_1\\2024-12-11_18-31-46_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\test_1\\2024-12-11_18-31-46_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\test_1\\2024-12-11_18-31-46_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "all_features = [\n",
    "            's1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_nouns', 's1_n_adjectives', 's1_n_adverbs', \n",
    "            's2_n_words', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_nouns', 's2_n_adjectives', 's2_n_adverbs', \n",
    "            'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', \n",
    "            'jaccard_all_words', 'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives', 'jaccard_adverbs',\n",
    "            \n",
    "            'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "\n",
    "            'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity',\n",
    "            'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity',\n",
    "            'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity',\n",
    "            'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity',\n",
    "            'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity',\n",
    "\n",
    "            'lemma_diversity', 'shared_lemmas_ratio', 'lemma_jackard_similarity', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient',\n",
    "            'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity'\n",
    "            ]\n",
    "\n",
    "def extract_features(feature_set, drop_features):\n",
    "    return [f for f in feature_set if f not in drop_features]\n",
    "\n",
    "model_trainer = ModelTrainer()\n",
    "print(f\"Total feaures: {len(all_features)}\")\n",
    "thresholds = [0.7, 0.75, 0.80, 0.85, 0.9, 0.95, 1]\n",
    "for threshold in thresholds:\n",
    "    print(f'\\nDropping columns with correlation above {threshold}')\n",
    "    dropped_columns = drop_highly_correlated_features(df_train, threshold)\n",
    "    print(f'Columns dropped: {len(dropped_columns)}')\n",
    "    new_columns = extract_features(all_features, dropped_columns)\n",
    "    print(f'Remaining columns: {len(new_columns)}')\n",
    "    best_rf_model_lex, rf_params_lex, metrics_lex, mean_correlation_rf_lex = evaluate_rf_model(\n",
    "        model_trainer, \n",
    "        original_df_train, \n",
    "        original_df_test, \n",
    "        new_columns, \n",
    "        'gs', \n",
    "        PREDICTED_SAVE_PATH,\n",
    "        f\"test_{threshold}\",\n",
    "        10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of features dropped: 32\n",
      "Remaining features after dropping highly correlated ones: Index(['s1_n_words', 's1_n_verbs_tot', 's2_n_verbs_tot', 's1_n_verbs_pres',\n",
      "       's2_n_verbs_pres', 's1_n_verbs_past', 's2_n_verbs_past',\n",
      "       's1_n_adjectives', 's1_n_adverbs', 's2_n_adverbs', 'dif_n_words',\n",
      "       'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past',\n",
      "       'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', 'jaccard_all_words',\n",
      "       'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives',\n",
      "       'jaccard_adverbs', 'all_all_shared_synsets_count',\n",
      "       'all_all_avg_synset_similarity', 'all_verb_shared_synsets_ratio',\n",
      "       'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
      "       'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity',\n",
      "       'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio',\n",
      "       'all_adj_max_synset_similarity', 'all_adv_shared_synsets_count',\n",
      "       'all_adv_shared_synsets_ratio', 'all_adv_max_synset_similarity',\n",
      "       'best_all_avg_synset_similarity', 'best_verb_shared_synsets_count',\n",
      "       'best_verb_max_synset_similarity', 'best_adj_shared_synsets_count',\n",
      "       'best_adj_avg_synset_similarity', 'best_adv_shared_synsets_count',\n",
      "       'best_adv_max_synset_similarity', 'avg_lemma_similarity',\n",
      "       'lemma_bigram_overlap', 'lemma_edit_distance', 'proportion_s2_in_s1',\n",
      "       'lemma_position_similarity'],\n",
      "      dtype='object')\n",
      "\n",
      "Number of remaining features after dropping highly correlated ones: 47\n",
      "\n",
      "Percentage of features removed: 40.51%\n"
     ]
    }
   ],
   "source": [
    "BEST_TRESHOLD = 0.8\n",
    "# Initial number of columns\n",
    "total_cols = len(df_train.columns)\n",
    "\n",
    "# Find columns to drop from df_train and df_test\n",
    "dropped_columns = drop_highly_correlated_features(df_train, BEST_TRESHOLD)\n",
    "\n",
    "df_train_trimmed = df_train.drop(columns=dropped_columns)\n",
    "df_test_trimmed = df_test.drop(columns=dropped_columns)\n",
    "\n",
    "# Statistics\n",
    "print(f'\\nNumber of features dropped: {len(dropped_columns)}')\n",
    "print(\"Remaining features after dropping highly correlated ones:\", df_train_trimmed.columns)\n",
    "print(f'\\nNumber of remaining features after dropping highly correlated ones: {len(df_train_trimmed.columns)}')\n",
    "print(f'\\nPercentage of features removed: {(total_cols - len(df_train_trimmed.columns)) / total_cols * 100:.2f}%')\n",
    "\n",
    "# Print shape of df_test to verify consistency\n",
    "# print(f'\\nShape of df_test after column removal: {df_test_trimmed.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Used Feature Sets (Prior to Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature sets for the analysis\n",
    "PoS_features = [\n",
    "            's1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_nouns', 's1_n_adjectives', 's1_n_adverbs', \n",
    "            's2_n_words', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_nouns', 's2_n_adjectives', 's2_n_adverbs', \n",
    "            'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', \n",
    "            'jaccard_all_words', 'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives', 'jaccard_adverbs',\n",
    "            ]\n",
    "\n",
    "synset_features = [\n",
    "            'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "\n",
    "            'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity',\n",
    "            'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity',\n",
    "            'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity',\n",
    "            'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity',\n",
    "            'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity',\n",
    "            ]\n",
    "\n",
    "lemma_features = [\n",
    "            'lemma_diversity', 'shared_lemmas_ratio', 'lemma_jackard_similarity', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient',\n",
    "            'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity'\n",
    "            ]\n",
    "\n",
    "lexical_features = [\n",
    "            'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "\n",
    "            'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity',\n",
    "            'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity',\n",
    "            'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity',\n",
    "            'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity',\n",
    "            'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity',\n",
    "\n",
    "            'lemma_diversity', 'shared_lemmas_ratio', 'lemma_jackard_similarity', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient',\n",
    "            'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity'\n",
    "]\n",
    "\n",
    "# Organize feature sets into labeled tuples\n",
    "features_sets = [\n",
    "    ('All', all_features),\n",
    "    ('Synsets', synset_features),\n",
    "    ('Lemmas', lemma_features),\n",
    "    ('PoS (Syntactic)', PoS_features),\n",
    "    ('Lexical', lexical_features),\n",
    "]\n",
    "\n",
    "# File sets for the analysis\n",
    "files_sets = [\n",
    "    ('SMTeuroparl', ['SMTeuroparl'], ['SMTeuroparl', 'surprise.OnWN', 'surprise.SMTnews']),\n",
    "    ('MSRvid', ['MSRvid'], ['MSRvid', 'surprise.OnWN', 'surprise.SMTnews']),\n",
    "    ('MSRpar', ['MSRpar'], ['MSRpar', 'surprise.OnWN', 'surprise.SMTnews']),\n",
    "    ('All', ['SMTeuroparl', 'MSRvid', 'MSRpar'], ['SMTeuroparl', 'MSRvid', 'MSRpar', 'surprise.OnWN', 'surprise.SMTnews'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 79 features\n",
      "['s1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_nouns', 's1_n_adjectives', 's1_n_adverbs', 's2_n_words', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_nouns', 's2_n_adjectives', 's2_n_adverbs', 'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', 'jaccard_all_words', 'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives', 'jaccard_adverbs', 'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity', 'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity', 'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity', 'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity', 'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity', 'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity', 'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity', 'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity', 'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity', 'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity', 'lemma_diversity', 'shared_lemmas_ratio', 'lemma_jackard_similarity', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient', 'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity']\n",
      "All: 47 features\n",
      "['s1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_adjectives', 's1_n_adverbs', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_adverbs', 'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', 'jaccard_all_words', 'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives', 'jaccard_adverbs', 'all_all_shared_synsets_count', 'all_all_avg_synset_similarity', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_max_synset_similarity', 'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_max_synset_similarity', 'best_all_avg_synset_similarity', 'best_verb_shared_synsets_count', 'best_verb_max_synset_similarity', 'best_adj_shared_synsets_count', 'best_adj_avg_synset_similarity', 'best_adv_shared_synsets_count', 'best_adv_max_synset_similarity', 'avg_lemma_similarity', 'lemma_bigram_overlap', 'lemma_edit_distance', 'proportion_s2_in_s1', 'lemma_position_similarity']\n",
      "PoS (Syntactic): 22 features\n",
      "['s1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_adjectives', 's1_n_adverbs', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_adverbs', 'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', 'jaccard_all_words', 'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives', 'jaccard_adverbs']\n",
      "Synsets: 20 features\n",
      "['all_all_shared_synsets_count', 'all_all_avg_synset_similarity', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_max_synset_similarity', 'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_max_synset_similarity', 'best_all_avg_synset_similarity', 'best_verb_shared_synsets_count', 'best_verb_max_synset_similarity', 'best_adj_shared_synsets_count', 'best_adj_avg_synset_similarity', 'best_adv_shared_synsets_count', 'best_adv_max_synset_similarity']\n",
      "Lemmas: 5 features\n",
      "['avg_lemma_similarity', 'lemma_bigram_overlap', 'lemma_edit_distance', 'proportion_s2_in_s1', 'lemma_position_similarity']\n",
      "Lexical: 25 features\n",
      "['all_all_shared_synsets_count', 'all_all_avg_synset_similarity', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_max_synset_similarity', 'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_max_synset_similarity', 'best_all_avg_synset_similarity', 'best_verb_shared_synsets_count', 'best_verb_max_synset_similarity', 'best_adj_shared_synsets_count', 'best_adj_avg_synset_similarity', 'best_adv_shared_synsets_count', 'best_adv_max_synset_similarity', 'avg_lemma_similarity', 'lemma_bigram_overlap', 'lemma_edit_distance', 'proportion_s2_in_s1', 'lemma_position_similarity']\n"
     ]
    }
   ],
   "source": [
    "# Pull remaining feature column names from df into \"all_features\"\n",
    "remaining_features = df_train_trimmed.columns.tolist()\n",
    "\n",
    "def filter_features(feature_set, remaining_features):\n",
    "    return [f for f in feature_set if f in remaining_features]\n",
    "\n",
    "# Create new filtered feature groups\n",
    "new_all_features = filter_features(all_features, remaining_features)\n",
    "new_PoS_features = filter_features(PoS_features, remaining_features)\n",
    "new_synset_features = filter_features(synset_features, remaining_features)\n",
    "new_lemma_features = filter_features(lemma_features, remaining_features)\n",
    "new_lexical_features = filter_features(lexical_features, remaining_features)\n",
    "\n",
    "filtered_feature_sets = [\n",
    "    ('Original', all_features),\n",
    "    ('All', new_all_features),\n",
    "    ('PoS (Syntactic)', new_PoS_features),\n",
    "    ('Synsets', new_synset_features),\n",
    "    ('Lemmas', new_lemma_features),\n",
    "    ('Lexical', new_lexical_features),\n",
    "]\n",
    "\n",
    "# Display remaining features in each feature set\n",
    "for name, features in filtered_feature_sets:\n",
    "    print(f\"{name}: {len(features)} features\")\n",
    "    print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the first 4 columns from df_train to df at the beginning\n",
    "df_train_trimmed = pd.concat([original_df_train[original_df_train.columns[:4]], df_train_trimmed], axis=1)\n",
    "df_test_trimmed = pd.concat([original_df_test[original_df_test.columns[:4]], df_test_trimmed], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer()\n",
    "N_ITERATIONS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the features (Not trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7514347843502969\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7504323834995141\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_original\\2024-12-11_18-37-57_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_original\\2024-12-11_18-37-57_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_original\\2024-12-11_18-37-57_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "best_rf_model_lex, rf_params_lex, metrics_lex, mean_correlation_rf_lex = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    original_df_train, \n",
    "    original_df_test, \n",
    "    all_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_original\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=f\"{PREDICTED_SAVE_PATH}/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"Original\",\n",
    "    metrics=metrics_lex,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_original.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimmed features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7140327037401144\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7139252144670927\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_synset\\2024-12-11_18-38-48_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_synset\\2024-12-11_18-38-48_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_synset\\2024-12-11_18-38-48_test_data.xlsx\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': True, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.668500072605863\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.6688331779502276\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_lemma\\2024-12-11_18-39-19_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_lemma\\2024-12-11_18-39-19_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_lemma\\2024-12-11_18-39-19_test_data.xlsx\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.742642731968866\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7416332152356934\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_lexical\\2024-12-11_18-40-13_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_lexical\\2024-12-11_18-40-13_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_lexical\\2024-12-11_18-40-13_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Synset based (semantic) features\n",
    "best_rf_model_lex, rf_params_lex, metrics_lex, mean_correlation_rf_lex = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    df_train_trimmed, \n",
    "    df_test_trimmed, \n",
    "    new_synset_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_synset\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=f\"{PREDICTED_SAVE_PATH}/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"Synsets\",\n",
    "    metrics=metrics_lex,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_sybnset.csv\"\n",
    ")\n",
    "\n",
    "# Lemma based features\n",
    "best_rf_model_lex, rf_params_lex, metrics_lex, mean_correlation_rf_lex = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    df_train_trimmed, \n",
    "    df_test_trimmed, \n",
    "    new_lemma_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_lemma\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=f\"{PREDICTED_SAVE_PATH}/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"Lemma\",\n",
    "    metrics=metrics_lex,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_lemma.csv\"\n",
    ")\n",
    "\n",
    "# All syntactic features\n",
    "best_rf_model_lex, rf_params_lex, metrics_lex, mean_correlation_rf_lex = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    df_train_trimmed, \n",
    "    df_test_trimmed, \n",
    "    new_lexical_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_lexical\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=f\"{PREDICTED_SAVE_PATH}/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"Lexical\",\n",
    "    metrics=metrics_lex,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_lexical.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntactical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.6872474376830605\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.6845818162480091\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_syn\\2024-12-11_18-41-28_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_syn\\2024-12-11_18-41-28_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_syn\\2024-12-11_18-41-28_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "best_rf_model_pos, rf_params_pos, metrics_pos, mean_correlation_rf_pos = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    df_train_trimmed, \n",
    "    df_test_trimmed, \n",
    "    new_PoS_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_syn\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=f\"{PREDICTED_SAVE_PATH}/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"Syntactical\",\n",
    "    metrics=metrics_pos,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_syn.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All trimmed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7627933471204229\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7608256233940128\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_all\\2024-12-11_18-42-58_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_all\\2024-12-11_18-42-58_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_all\\2024-12-11_18-42-58_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "best_rf_model_all, rf_params_all, metrics_rf_all, mean_correlation_rf_all = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    df_train_trimmed, \n",
    "    df_test_trimmed, \n",
    "    new_all_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_all\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=f\"{PREDICTED_SAVE_PATH}/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"All\",\n",
    "    metrics=metrics_rf_all,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_all.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots have been saved to ../data/test/03_predicted//plots/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_plots_from_metrics(f\"{PREDICTED_SAVE_PATH}/results.csv\", save_path=f\"{PREDICTED_SAVE_PATH}/plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Additional Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'batch_size': 16, 'epochs': 100, 'model__hidden_layers': 2, 'model__learning_rate': 0.001, 'model__neurons': 10}\n",
      "\n",
      " Pearson correlation for the best NN model: 0.6666183714193672\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_nn_all.csv\\2024-12-11_18-47-05_test_data.csv\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_nn_all.csv\\2024-12-11_18-47-05_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Train the best NN model using all features\n",
    "best_nn_model = model_trainer.train_NN(df_train_trimmed, new_all_features, 'gs')\n",
    "\n",
    "# Predict the test data\n",
    "df_test_trimmed['predicted_nn'] = best_nn_model.predict(df_test_trimmed[new_all_features])\n",
    "\n",
    "# Calculate the Pearson correlation for the best NN model\n",
    "correlation_nn = pearsonr(df_test_trimmed['gs'], df_test_trimmed['predicted_nn'])[0]\n",
    "print(f'\\n Pearson correlation for the best NN model: {correlation_nn}')\n",
    "\n",
    "# Save the predictions\n",
    "save_predictions(df_test, PREDICTED_SAVE_PATH, 'predicted_nn_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "\n",
      " Pearson correlation for the best MLP model: 0.7120592736152007\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_mlp_all.csv\\2024-12-11_18-47-45_test_data.csv\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_mlp_all.csv\\2024-12-11_18-47-45_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Train the best MLP model using all features\n",
    "best_mlp_model = model_trainer.train_MLP(df_train_trimmed, new_all_features, 'gs')\n",
    "\n",
    "# Predict the test data\n",
    "df_test_trimmed['predicted_mlp'] = best_mlp_model.predict(df_test_trimmed[new_all_features])\n",
    "\n",
    "# Calculate the Pearson correlation for the best NN model\n",
    "correlation_mlp = pearsonr(df_test_trimmed['gs'], df_test_trimmed['predicted_mlp'])[0]\n",
    "print(f'\\n Pearson correlation for the best MLP model: {correlation_mlp}')\n",
    "\n",
    "# Save the predictions\n",
    "save_predictions(df_test, PREDICTED_SAVE_PATH, 'predicted_mlp_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result analysis to go here. This could go into a separate file (notebook or .md file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion to go here. This could go into a separate file (notebook or .md file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
