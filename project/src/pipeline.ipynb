{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Textual Similarity Project\n",
    "\n",
    "## Authors\n",
    "- Kacper Poniatowski\n",
    "- Pau Blanco\n",
    "\n",
    "## Introduction\n",
    "This notebook is dedicated to the exploration and implementation of methods for Semantic Textual Similarity (STS) as part of the end-of-semester project in the Introduction to Human Language Technologies (IHLT) course. This project revolves around Task 6 of the 'SamEval 2012' competition, which focuses on measuring the similarity of sentence pairs within text documents.\n",
    "\n",
    "The pipeline outlined in this notebook consists of multiple stages:\n",
    "1. Data Preparation and Preprocessing: Loading datasets and cleaning text for analysis.\n",
    "2. Feature Extraction. Generating features that explore lexical, syntactic and other additional dimensions to capture semantic similarity.\n",
    "3. Model Training and Evaluation: Various models are trained on the dataset and then evaluated, with primary focus on Random Forest (RF).\n",
    "4. Result Analysis: Obtained results are interpreted and visualised.\n",
    "\n",
    "This project adheres to the constraint outlined within the assignment brief: avoiding the use of pre-trained word embeddings like BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Reqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Force auto-reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from utils import load_data, evaluate_rf_model, drop_highly_correlated_features, update_results_csv, generate_plots_from_metrics\n",
    "from models import ModelTrainer\n",
    "from feature_extraction import FeatureExtractor\n",
    "from utils import save_predictions\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constant paths used throughout notebook\n",
    "TRAIN_PATH = '../data/train/01_raw/'\n",
    "TRAIN_GS_PATH = '../data/train/scores/'\n",
    "TEST_PATH = '../data/test/01_raw/'\n",
    "TEST_GS_PATH = '../data/test/scores/'\n",
    "TRAIN_SAVE_PATH = '../data/train/02_preprocessed/preprocessed_train_data.csv'\n",
    "TEST_SAVE_PATH = '../data/test/02_preprocessed/preprocessed_test_data.csv'\n",
    "PREDICTED_SAVE_PATH = '../data/test/03_predicted/'\n",
    "RESULTS_SAVE_PATH = \"project/test/03_predicted/results.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading train data\n",
      "\n",
      " Loading test data\n",
      "\n",
      " Train and test datasets loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "print('\\n Loading train data')\n",
    "all_train_files = ['SMTeuroparl', 'MSRvid', 'MSRpar']\n",
    "df_train = load_data(TRAIN_PATH, TRAIN_GS_PATH, all_train_files)\n",
    "\n",
    "# Load test data\n",
    "print('\\n Loading test data')\n",
    "all_test_files = ['SMTeuroparl', 'MSRvid', 'MSRpar', 'surprise.OnWN', 'surprise.SMTnews']\n",
    "df_test = load_data(TEST_PATH, TEST_GS_PATH, all_test_files)\n",
    "\n",
    "print('\\n Train and test datasets loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding POS based features...\n",
      "Adding synset-based features...\n",
      "Processed 2234 of 2234 rows (100%)      \n",
      "Adding lemma based features...\n",
      "Adding POS based features...\n",
      "Adding synset-based features...\n",
      "Processed 3108 of 3108 rows (100%)      \n",
      "Adding lemma based features...\n",
      "\n",
      " Features added to datasets successfully\n",
      "\n",
      " Train and test datasets saved as .csv files successfully\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Extract the desired features\n",
    "def add_features(dt):\n",
    "    feature_extractor.add_POS_statistics(dt)\n",
    "    feature_extractor.add_synset_statistics(dt)\n",
    "    feature_extractor.add_lemma_statistics(dt)\n",
    "\n",
    "# Add features to the training and test data\n",
    "add_features(df_train)\n",
    "add_features(df_test)\n",
    "\n",
    "print('\\n Features added to datasets successfully')\n",
    "\n",
    "# Save df_train and df_test to respective files - this is done to avoid re-running the\n",
    "# feature extraction process each time as synset extraction is computationally expensive\n",
    "df_test.to_csv(TEST_SAVE_PATH, index=False)\n",
    "df_train.to_csv(TRAIN_SAVE_PATH, index=False)\n",
    "\n",
    "print('\\n Train and test datasets saved as .csv files successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Score Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Existing Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train and test datasets loaded from .csv files successfully\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_SAVE_PATH is None or TEST_SAVE_PATH is None:\n",
    "    raise ValueError('TRAIN_SAVE_PATH and TEST_SAVE_PATH must be defined')\n",
    "\n",
    "if not os.path.exists(TRAIN_SAVE_PATH):\n",
    "    raise FileNotFoundError(f'The file {TRAIN_SAVE_PATH} does not exist')\n",
    "\n",
    "if not os.path.exists(TEST_SAVE_PATH):\n",
    "    raise FileNotFoundError(f'The file {TEST_SAVE_PATH} does not exist')\n",
    "\n",
    "original_df_train = pd.read_csv(TRAIN_SAVE_PATH)\n",
    "original_df_test = pd.read_csv(TEST_SAVE_PATH)\n",
    "\n",
    "print('\\n Train and test datasets loaded from .csv files successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           s1_n_words  s2_n_words  s1_n_verbs_tot  \\\n",
      "s1_n_words                   1.000000    0.918735        0.758300   \n",
      "s2_n_words                   0.918735    1.000000        0.673713   \n",
      "s1_n_verbs_tot               0.758300    0.673713        1.000000   \n",
      "s2_n_verbs_tot               0.590479    0.698453        0.740191   \n",
      "s1_n_verbs_pres              0.246101    0.191457        0.571911   \n",
      "...                               ...         ...             ...   \n",
      "lemma_lcs_length             0.814533    0.793255        0.545660   \n",
      "lemma_edit_distance          0.781976    0.809274        0.570023   \n",
      "proportion_s1_in_s2          0.202598    0.231090        0.077001   \n",
      "proportion_s2_in_s1          0.298470    0.213312        0.158630   \n",
      "lemma_position_similarity    0.153036    0.110382        0.118022   \n",
      "\n",
      "                           s2_n_verbs_tot  s1_n_verbs_pres  s2_n_verbs_pres  \\\n",
      "s1_n_words                       0.590479         0.246101         0.113169   \n",
      "s2_n_words                       0.698453         0.191457         0.203622   \n",
      "s1_n_verbs_tot                   0.740191         0.571911         0.352796   \n",
      "s2_n_verbs_tot                   1.000000         0.359068         0.585477   \n",
      "s1_n_verbs_pres                  0.359068         1.000000         0.583186   \n",
      "...                                   ...              ...              ...   \n",
      "lemma_lcs_length                 0.470067         0.036863        -0.021604   \n",
      "lemma_edit_distance              0.542890         0.157074         0.102533   \n",
      "proportion_s1_in_s2              0.086514        -0.158674        -0.147527   \n",
      "proportion_s2_in_s1              0.070091        -0.122421        -0.165168   \n",
      "lemma_position_similarity        0.040985         0.016534        -0.049000   \n",
      "\n",
      "                           s1_n_verbs_past  s2_n_verbs_past  s1_n_nouns  \\\n",
      "s1_n_words                        0.541747         0.366501    0.879111   \n",
      "s2_n_words                        0.477406         0.426594    0.812494   \n",
      "s1_n_verbs_tot                    0.616441         0.373132    0.520241   \n",
      "s2_n_verbs_tot                    0.473997         0.555110    0.405833   \n",
      "s1_n_verbs_pres                  -0.088747        -0.161231    0.066217   \n",
      "...                                    ...              ...         ...   \n",
      "lemma_lcs_length                  0.519421         0.441736    0.812901   \n",
      "lemma_edit_distance               0.443265         0.373852    0.741351   \n",
      "proportion_s1_in_s2               0.210567         0.230728    0.230298   \n",
      "proportion_s2_in_s1               0.269901         0.213446    0.330817   \n",
      "lemma_position_similarity         0.090492         0.038153    0.128458   \n",
      "\n",
      "                           s2_n_nouns  ...  max_lemma_similarity  \\\n",
      "s1_n_words                   0.822095  ...              0.229858   \n",
      "s2_n_words                   0.884512  ...              0.150631   \n",
      "s1_n_verbs_tot               0.481373  ...              0.147050   \n",
      "s2_n_verbs_tot               0.451101  ...              0.017157   \n",
      "s1_n_verbs_pres              0.045344  ...             -0.036966   \n",
      "...                               ...  ...                   ...   \n",
      "lemma_lcs_length             0.808128  ...              0.301991   \n",
      "lemma_edit_distance          0.758161  ...              0.067852   \n",
      "proportion_s1_in_s2          0.283861  ...              0.543419   \n",
      "proportion_s2_in_s1          0.239628  ...              0.546156   \n",
      "lemma_position_similarity    0.109522  ...              0.828971   \n",
      "\n",
      "                           lemma_jackard_similarity  shared_lemma_count  \\\n",
      "s1_n_words                                 0.224797            0.823098   \n",
      "s2_n_words                                 0.204095            0.802170   \n",
      "s1_n_verbs_tot                             0.093844            0.546626   \n",
      "s2_n_verbs_tot                             0.066700            0.463084   \n",
      "s1_n_verbs_pres                           -0.138852            0.036443   \n",
      "...                                             ...                 ...   \n",
      "lemma_lcs_length                           0.552054            0.965594   \n",
      "lemma_edit_distance                       -0.085678            0.576657   \n",
      "proportion_s1_in_s2                        0.941852            0.575162   \n",
      "proportion_s2_in_s1                        0.936890            0.599770   \n",
      "lemma_position_similarity                  0.394989            0.214088   \n",
      "\n",
      "                           dice_coefficient  lemma_bigram_overlap  \\\n",
      "s1_n_words                         0.264632              0.169512   \n",
      "s2_n_words                         0.236947              0.153650   \n",
      "s1_n_verbs_tot                     0.125389              0.060910   \n",
      "s2_n_verbs_tot                     0.085188              0.051548   \n",
      "s1_n_verbs_pres                   -0.144292             -0.128608   \n",
      "...                                     ...                   ...   \n",
      "lemma_lcs_length                   0.582465              0.479089   \n",
      "lemma_edit_distance               -0.042950             -0.110112   \n",
      "proportion_s1_in_s2                0.962305              0.731835   \n",
      "proportion_s2_in_s1                0.959543              0.738100   \n",
      "lemma_position_similarity          0.499750              0.201317   \n",
      "\n",
      "                           lemma_lcs_length  lemma_edit_distance  \\\n",
      "s1_n_words                         0.814533             0.781976   \n",
      "s2_n_words                         0.793255             0.809274   \n",
      "s1_n_verbs_tot                     0.545660             0.570023   \n",
      "s2_n_verbs_tot                     0.470067             0.542890   \n",
      "s1_n_verbs_pres                    0.036863             0.157074   \n",
      "...                                     ...                  ...   \n",
      "lemma_lcs_length                   1.000000             0.508607   \n",
      "lemma_edit_distance                0.508607             1.000000   \n",
      "proportion_s1_in_s2                0.542896            -0.061796   \n",
      "proportion_s2_in_s1                0.568452            -0.020979   \n",
      "lemma_position_similarity          0.263695            -0.094165   \n",
      "\n",
      "                           proportion_s1_in_s2  proportion_s2_in_s1  \\\n",
      "s1_n_words                            0.202598             0.298470   \n",
      "s2_n_words                            0.231090             0.213312   \n",
      "s1_n_verbs_tot                        0.077001             0.158630   \n",
      "s2_n_verbs_tot                        0.086514             0.070091   \n",
      "s1_n_verbs_pres                      -0.158674            -0.122421   \n",
      "...                                        ...                  ...   \n",
      "lemma_lcs_length                      0.542896             0.568452   \n",
      "lemma_edit_distance                  -0.061796            -0.020979   \n",
      "proportion_s1_in_s2                   1.000000             0.852209   \n",
      "proportion_s2_in_s1                   0.852209             1.000000   \n",
      "lemma_position_similarity             0.484480             0.473087   \n",
      "\n",
      "                           lemma_position_similarity  \n",
      "s1_n_words                                  0.153036  \n",
      "s2_n_words                                  0.110382  \n",
      "s1_n_verbs_tot                              0.118022  \n",
      "s2_n_verbs_tot                              0.040985  \n",
      "s1_n_verbs_pres                             0.016534  \n",
      "...                                              ...  \n",
      "lemma_lcs_length                            0.263695  \n",
      "lemma_edit_distance                        -0.094165  \n",
      "proportion_s1_in_s2                         0.484480  \n",
      "proportion_s2_in_s1                         0.473087  \n",
      "lemma_position_similarity                   1.000000  \n",
      "\n",
      "[79 rows x 79 columns]\n",
      "\n",
      " Total number of columns (features) in the correlation matrix: 79\n"
     ]
    }
   ],
   "source": [
    "# Drop first 4 columns to remove non-numerical columns\n",
    "df_train = original_df_train.drop(original_df_train.columns[:4], axis=1)\n",
    "df_test = original_df_test.drop(original_df_test.columns[:4], axis=1)\n",
    "\n",
    "correlation_matrix = df_train.corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Total number of columns in the correlation matrix\n",
    "total_cols = correlation_matrix.shape[1]\n",
    "print(f'\\n Total number of columns (features) in the correlation matrix: {total_cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find best correlation threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feaures: 79\n",
      "\n",
      "Dropping columns with correlation above 0.7\n",
      "Columns dropped: 41\n",
      "Remaining columns: 38\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.750920528636134\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7513897840948833\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\test_0.7\\2024-12-11_18-25-38_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\test_0.7\\2024-12-11_18-25-38_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\test_0.7\\2024-12-11_18-25-38_test_data.xlsx\n",
      "\n",
      "Dropping columns with correlation above 0.75\n",
      "Columns dropped: 36\n",
      "Remaining columns: 43\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7623454002064807\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7591629084486324\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\test_0.75\\2024-12-11_18-26-26_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\test_0.75\\2024-12-11_18-26-26_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\test_0.75\\2024-12-11_18-26-26_test_data.xlsx\n",
      "\n",
      "Dropping columns with correlation above 0.8\n",
      "Columns dropped: 32\n",
      "Remaining columns: 47\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7634948927164802\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.762263090528221\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\test_0.8\\2024-12-11_18-27-29_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\test_0.8\\2024-12-11_18-27-29_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\test_0.8\\2024-12-11_18-27-29_test_data.xlsx\n",
      "\n",
      "Dropping columns with correlation above 0.85\n",
      "Columns dropped: 26\n",
      "Remaining columns: 53\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7520333671236472\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7531644600918147\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\test_0.85\\2024-12-11_18-28-37_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\test_0.85\\2024-12-11_18-28-37_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\test_0.85\\2024-12-11_18-28-37_test_data.xlsx\n",
      "\n",
      "Dropping columns with correlation above 0.9\n",
      "Columns dropped: 14\n",
      "Remaining columns: 65\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7580475730311692\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7590360612800903\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\test_0.9\\2024-12-11_18-29-41_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\test_0.9\\2024-12-11_18-29-41_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\test_0.9\\2024-12-11_18-29-41_test_data.xlsx\n",
      "\n",
      "Dropping columns with correlation above 0.95\n",
      "Columns dropped: 10\n",
      "Remaining columns: 69\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7612824081096524\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7599192290067266\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\test_0.95\\2024-12-11_18-30-57_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\test_0.95\\2024-12-11_18-30-57_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\test_0.95\\2024-12-11_18-30-57_test_data.xlsx\n",
      "\n",
      "Dropping columns with correlation above 1\n",
      "Columns dropped: 0\n",
      "Remaining columns: 79\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7524110007850136\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7524362569046683\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\test_1\\2024-12-11_18-31-46_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\test_1\\2024-12-11_18-31-46_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\test_1\\2024-12-11_18-31-46_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "all_features = [\n",
    "            's1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_nouns', 's1_n_adjectives', 's1_n_adverbs', \n",
    "            's2_n_words', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_nouns', 's2_n_adjectives', 's2_n_adverbs', \n",
    "            'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', \n",
    "            'jaccard_all_words', 'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives', 'jaccard_adverbs',\n",
    "            \n",
    "            'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "\n",
    "            'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity',\n",
    "            'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity',\n",
    "            'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity',\n",
    "            'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity',\n",
    "            'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity',\n",
    "\n",
    "            'lemma_diversity', 'shared_lemmas_ratio', 'lemma_jackard_similarity', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient',\n",
    "            'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity'\n",
    "            ]\n",
    "\n",
    "def extract_features(feature_set, drop_features):\n",
    "    return [f for f in feature_set if f not in drop_features]\n",
    "\n",
    "model_trainer = ModelTrainer()\n",
    "print(f\"Total feaures: {len(all_features)}\")\n",
    "thresholds = [0.7, 0.75, 0.80, 0.85, 0.9, 0.95, 1]\n",
    "for threshold in thresholds:\n",
    "    print(f'\\nDropping columns with correlation above {threshold}')\n",
    "    dropped_columns = drop_highly_correlated_features(df_train, threshold)\n",
    "    print(f'Columns dropped: {len(dropped_columns)}')\n",
    "    new_columns = extract_features(all_features, dropped_columns)\n",
    "    print(f'Remaining columns: {len(new_columns)}')\n",
    "    best_rf_model_lex, rf_params_lex, metrics_lex, mean_correlation_rf_lex = evaluate_rf_model(\n",
    "        model_trainer, \n",
    "        original_df_train, \n",
    "        original_df_test, \n",
    "        new_columns, \n",
    "        'gs', \n",
    "        PREDICTED_SAVE_PATH,\n",
    "        f\"test_{threshold}\",\n",
    "        10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of features dropped: 32\n",
      "Remaining features after dropping highly correlated ones: Index(['s1_n_words', 's1_n_verbs_tot', 's2_n_verbs_tot', 's1_n_verbs_pres',\n",
      "       's2_n_verbs_pres', 's1_n_verbs_past', 's2_n_verbs_past',\n",
      "       's1_n_adjectives', 's1_n_adverbs', 's2_n_adverbs', 'dif_n_words',\n",
      "       'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past',\n",
      "       'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', 'jaccard_all_words',\n",
      "       'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives',\n",
      "       'jaccard_adverbs', 'all_all_shared_synsets_count',\n",
      "       'all_all_avg_synset_similarity', 'all_verb_shared_synsets_ratio',\n",
      "       'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
      "       'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity',\n",
      "       'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio',\n",
      "       'all_adj_max_synset_similarity', 'all_adv_shared_synsets_count',\n",
      "       'all_adv_shared_synsets_ratio', 'all_adv_max_synset_similarity',\n",
      "       'best_all_avg_synset_similarity', 'best_verb_shared_synsets_count',\n",
      "       'best_verb_max_synset_similarity', 'best_adj_shared_synsets_count',\n",
      "       'best_adj_avg_synset_similarity', 'best_adv_shared_synsets_count',\n",
      "       'best_adv_max_synset_similarity', 'avg_lemma_similarity',\n",
      "       'lemma_bigram_overlap', 'lemma_edit_distance', 'proportion_s2_in_s1',\n",
      "       'lemma_position_similarity'],\n",
      "      dtype='object')\n",
      "\n",
      "Number of remaining features after dropping highly correlated ones: 47\n",
      "\n",
      "Percentage of features removed: 40.51%\n"
     ]
    }
   ],
   "source": [
    "BEST_TRESHOLD = 0.8\n",
    "# Initial number of columns\n",
    "total_cols = len(df_train.columns)\n",
    "\n",
    "# Find columns to drop from df_train and df_test\n",
    "dropped_columns = drop_highly_correlated_features(df_train, BEST_TRESHOLD)\n",
    "\n",
    "df_train_trimmed = df_train.drop(columns=dropped_columns)\n",
    "df_test_trimmed = df_test.drop(columns=dropped_columns)\n",
    "\n",
    "# Statistics\n",
    "print(f'\\nNumber of features dropped: {len(dropped_columns)}')\n",
    "print(\"Remaining features after dropping highly correlated ones:\", df_train_trimmed.columns)\n",
    "print(f'\\nNumber of remaining features after dropping highly correlated ones: {len(df_train_trimmed.columns)}')\n",
    "print(f'\\nPercentage of features removed: {(total_cols - len(df_train_trimmed.columns)) / total_cols * 100:.2f}%')\n",
    "\n",
    "# Print shape of df_test to verify consistency\n",
    "# print(f'\\nShape of df_test after column removal: {df_test_trimmed.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Used Feature Sets (Prior to Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature sets for the analysis\n",
    "PoS_features = [\n",
    "            's1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_nouns', 's1_n_adjectives', 's1_n_adverbs', \n",
    "            's2_n_words', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_nouns', 's2_n_adjectives', 's2_n_adverbs', \n",
    "            'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', \n",
    "            'jaccard_all_words', 'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives', 'jaccard_adverbs',\n",
    "            ]\n",
    "\n",
    "synset_features = [\n",
    "            'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "\n",
    "            'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity',\n",
    "            'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity',\n",
    "            'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity',\n",
    "            'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity',\n",
    "            'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity',\n",
    "            ]\n",
    "\n",
    "lemma_features = [\n",
    "            'lemma_diversity', 'shared_lemmas_ratio', 'lemma_jackard_similarity', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient',\n",
    "            'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity'\n",
    "            ]\n",
    "\n",
    "lexical_features = [\n",
    "            'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "\n",
    "            'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity',\n",
    "            'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity',\n",
    "            'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity',\n",
    "            'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity',\n",
    "            'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity',\n",
    "\n",
    "            'lemma_diversity', 'shared_lemmas_ratio', 'lemma_jackard_similarity', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient',\n",
    "            'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity'\n",
    "]\n",
    "\n",
    "# Organize feature sets into labeled tuples\n",
    "features_sets = [\n",
    "    ('All', all_features),\n",
    "    ('Synsets', synset_features),\n",
    "    ('Lemmas', lemma_features),\n",
    "    ('PoS (Syntactic)', PoS_features),\n",
    "    ('Lexical', lexical_features),\n",
    "]\n",
    "\n",
    "# File sets for the analysis\n",
    "files_sets = [\n",
    "    ('SMTeuroparl', ['SMTeuroparl'], ['SMTeuroparl', 'surprise.OnWN', 'surprise.SMTnews']),\n",
    "    ('MSRvid', ['MSRvid'], ['MSRvid', 'surprise.OnWN', 'surprise.SMTnews']),\n",
    "    ('MSRpar', ['MSRpar'], ['MSRpar', 'surprise.OnWN', 'surprise.SMTnews']),\n",
    "    ('All', ['SMTeuroparl', 'MSRvid', 'MSRpar'], ['SMTeuroparl', 'MSRvid', 'MSRpar', 'surprise.OnWN', 'surprise.SMTnews'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 79 features\n",
      "['s1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_nouns', 's1_n_adjectives', 's1_n_adverbs', 's2_n_words', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_nouns', 's2_n_adjectives', 's2_n_adverbs', 'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', 'jaccard_all_words', 'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives', 'jaccard_adverbs', 'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity', 'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity', 'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity', 'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity', 'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity', 'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity', 'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity', 'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity', 'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity', 'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity', 'lemma_diversity', 'shared_lemmas_ratio', 'lemma_jackard_similarity', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient', 'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity']\n",
      "All: 47 features\n",
      "['s1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_adjectives', 's1_n_adverbs', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_adverbs', 'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', 'jaccard_all_words', 'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives', 'jaccard_adverbs', 'all_all_shared_synsets_count', 'all_all_avg_synset_similarity', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_max_synset_similarity', 'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_max_synset_similarity', 'best_all_avg_synset_similarity', 'best_verb_shared_synsets_count', 'best_verb_max_synset_similarity', 'best_adj_shared_synsets_count', 'best_adj_avg_synset_similarity', 'best_adv_shared_synsets_count', 'best_adv_max_synset_similarity', 'avg_lemma_similarity', 'lemma_bigram_overlap', 'lemma_edit_distance', 'proportion_s2_in_s1', 'lemma_position_similarity']\n",
      "PoS (Syntactic): 22 features\n",
      "['s1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_adjectives', 's1_n_adverbs', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_adverbs', 'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', 'jaccard_all_words', 'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives', 'jaccard_adverbs']\n",
      "Synsets: 20 features\n",
      "['all_all_shared_synsets_count', 'all_all_avg_synset_similarity', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_max_synset_similarity', 'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_max_synset_similarity', 'best_all_avg_synset_similarity', 'best_verb_shared_synsets_count', 'best_verb_max_synset_similarity', 'best_adj_shared_synsets_count', 'best_adj_avg_synset_similarity', 'best_adv_shared_synsets_count', 'best_adv_max_synset_similarity']\n",
      "Lemmas: 5 features\n",
      "['avg_lemma_similarity', 'lemma_bigram_overlap', 'lemma_edit_distance', 'proportion_s2_in_s1', 'lemma_position_similarity']\n",
      "Lexical: 25 features\n",
      "['all_all_shared_synsets_count', 'all_all_avg_synset_similarity', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_max_synset_similarity', 'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_max_synset_similarity', 'best_all_avg_synset_similarity', 'best_verb_shared_synsets_count', 'best_verb_max_synset_similarity', 'best_adj_shared_synsets_count', 'best_adj_avg_synset_similarity', 'best_adv_shared_synsets_count', 'best_adv_max_synset_similarity', 'avg_lemma_similarity', 'lemma_bigram_overlap', 'lemma_edit_distance', 'proportion_s2_in_s1', 'lemma_position_similarity']\n"
     ]
    }
   ],
   "source": [
    "# Pull remaining feature column names from df into \"all_features\"\n",
    "remaining_features = df_train_trimmed.columns.tolist()\n",
    "\n",
    "def filter_features(feature_set, remaining_features):\n",
    "    return [f for f in feature_set if f in remaining_features]\n",
    "\n",
    "# Create new filtered feature groups\n",
    "new_all_features = filter_features(all_features, remaining_features)\n",
    "new_PoS_features = filter_features(PoS_features, remaining_features)\n",
    "new_synset_features = filter_features(synset_features, remaining_features)\n",
    "new_lemma_features = filter_features(lemma_features, remaining_features)\n",
    "new_lexical_features = filter_features(lexical_features, remaining_features)\n",
    "\n",
    "filtered_feature_sets = [\n",
    "    ('Original', all_features),\n",
    "    ('All', new_all_features),\n",
    "    ('PoS (Syntactic)', new_PoS_features),\n",
    "    ('Synsets', new_synset_features),\n",
    "    ('Lemmas', new_lemma_features),\n",
    "    ('Lexical', new_lexical_features),\n",
    "]\n",
    "\n",
    "# Display remaining features in each feature set\n",
    "for name, features in filtered_feature_sets:\n",
    "    print(f\"{name}: {len(features)} features\")\n",
    "    print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the first 4 columns from df_train to df at the beginning\n",
    "df_train_trimmed = pd.concat([original_df_train[original_df_train.columns[:4]], df_train_trimmed], axis=1)\n",
    "df_test_trimmed = pd.concat([original_df_test[original_df_test.columns[:4]], df_test_trimmed], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer()\n",
    "N_ITERATIONS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the features (Not trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7514347843502969\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7504323834995141\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_original\\2024-12-11_18-37-57_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_original\\2024-12-11_18-37-57_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_original\\2024-12-11_18-37-57_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "best_rf_model_lex, rf_params_lex, metrics_lex, mean_correlation_rf_lex = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    original_df_train, \n",
    "    original_df_test, \n",
    "    all_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_original\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=f\"{PREDICTED_SAVE_PATH}/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"Original\",\n",
    "    metrics=metrics_lex,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_original.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimmed features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7140327037401144\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7139252144670927\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_synset\\2024-12-11_18-38-48_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_synset\\2024-12-11_18-38-48_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_synset\\2024-12-11_18-38-48_test_data.xlsx\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': True, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.668500072605863\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.6688331779502276\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_lemma\\2024-12-11_18-39-19_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_lemma\\2024-12-11_18-39-19_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_lemma\\2024-12-11_18-39-19_test_data.xlsx\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.742642731968866\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7416332152356934\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_lexical\\2024-12-11_18-40-13_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_lexical\\2024-12-11_18-40-13_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_lexical\\2024-12-11_18-40-13_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Synset based (semantic) features\n",
    "best_rf_model_lex, rf_params_lex, metrics_lex, mean_correlation_rf_lex = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    df_train_trimmed, \n",
    "    df_test_trimmed, \n",
    "    new_synset_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_synset\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=f\"{PREDICTED_SAVE_PATH}/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"Synsets\",\n",
    "    metrics=metrics_lex,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_sybnset.csv\"\n",
    ")\n",
    "\n",
    "# Lemma based features\n",
    "best_rf_model_lex, rf_params_lex, metrics_lex, mean_correlation_rf_lex = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    df_train_trimmed, \n",
    "    df_test_trimmed, \n",
    "    new_lemma_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_lemma\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=f\"{PREDICTED_SAVE_PATH}/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"Lemma\",\n",
    "    metrics=metrics_lex,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_lemma.csv\"\n",
    ")\n",
    "\n",
    "# All lexical features\n",
    "best_rf_model_lex, rf_params_lex, metrics_lex, mean_correlation_rf_lex = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    df_train_trimmed, \n",
    "    df_test_trimmed, \n",
    "    new_lexical_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_lexical\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=f\"{PREDICTED_SAVE_PATH}/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"Lexical\",\n",
    "    metrics=metrics_lex,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_lexical.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntactical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.6872474376830605\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.6845818162480091\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_syn\\2024-12-11_18-41-28_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_syn\\2024-12-11_18-41-28_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_syn\\2024-12-11_18-41-28_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "best_rf_model_pos, rf_params_pos, metrics_pos, mean_correlation_rf_pos = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    df_train_trimmed, \n",
    "    df_test_trimmed, \n",
    "    new_PoS_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_syn\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=f\"{PREDICTED_SAVE_PATH}/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"Syntactical\",\n",
    "    metrics=metrics_pos,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_syn.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All trimmed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7627933471204229\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7608256233940128\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_all\\2024-12-11_18-42-58_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_all\\2024-12-11_18-42-58_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_all\\2024-12-11_18-42-58_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "best_rf_model_all, rf_params_all, metrics_rf_all, mean_correlation_rf_all = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    df_train_trimmed, \n",
    "    df_test_trimmed, \n",
    "    new_all_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_all\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=f\"{PREDICTED_SAVE_PATH}/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"All\",\n",
    "    metrics=metrics_rf_all,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_all.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots have been saved to ../data/test/03_predicted//plots/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_plots_from_metrics(f\"{PREDICTED_SAVE_PATH}/results.csv\", save_path=f\"{PREDICTED_SAVE_PATH}/plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Additional Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'batch_size': 16, 'epochs': 100, 'model__hidden_layers': 2, 'model__learning_rate': 0.001, 'model__neurons': 10}\n",
      "\n",
      " Pearson correlation for the best NN model: 0.6666183714193672\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_nn_all.csv\\2024-12-11_18-47-05_test_data.csv\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_nn_all.csv\\2024-12-11_18-47-05_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Train the best NN model using all features\n",
    "best_nn_model = model_trainer.train_NN(df_train_trimmed, new_all_features, 'gs')\n",
    "\n",
    "# Predict the test data\n",
    "df_test_trimmed['predicted_nn'] = best_nn_model.predict(df_test_trimmed[new_all_features])\n",
    "\n",
    "# Calculate the Pearson correlation for the best NN model\n",
    "correlation_nn = pearsonr(df_test_trimmed['gs'], df_test_trimmed['predicted_nn'])[0]\n",
    "print(f'\\n Pearson correlation for the best NN model: {correlation_nn}')\n",
    "\n",
    "# Save the predictions\n",
    "save_predictions(df_test, PREDICTED_SAVE_PATH, 'predicted_nn_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "\n",
      " Pearson correlation for the best MLP model: 0.7120592736152007\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_mlp_all.csv\\2024-12-11_18-47-45_test_data.csv\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_mlp_all.csv\\2024-12-11_18-47-45_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Train the best MLP model using all features\n",
    "best_mlp_model = model_trainer.train_MLP(df_train_trimmed, new_all_features, 'gs')\n",
    "\n",
    "# Predict the test data\n",
    "df_test_trimmed['predicted_mlp'] = best_mlp_model.predict(df_test_trimmed[new_all_features])\n",
    "\n",
    "# Calculate the Pearson correlation for the best NN model\n",
    "correlation_mlp = pearsonr(df_test_trimmed['gs'], df_test_trimmed['predicted_mlp'])[0]\n",
    "print(f'\\n Pearson correlation for the best MLP model: {correlation_mlp}')\n",
    "\n",
    "# Save the predictions\n",
    "save_predictions(df_test, PREDICTED_SAVE_PATH, 'predicted_mlp_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section discusses the obtained results from performing the various methods throughout the project. \n",
    "\n",
    "The primary evaluation metric used to evaluate the trained models is the Pearson correlation score, which in the context of this project, is used to measure how closely the similarity scores predicted by our trained models align with the ground truth scores provided to us. \n",
    "The score range is [-1, 1], where:\n",
    "- r = 1: the model is detecting paraphrase scores perfectly\n",
    "- r = 0: the model is failing to predict the paraphase scores.\n",
    "- r = -1: the model is inverting the ground truth score for each sentence pair.\n",
    "\n",
    "The key approaches that were compared are:\n",
    "1. Lexical features\n",
    "2. Syntactic features\n",
    "3. Combination of lexical and syntactic features\n",
    "4. Additional features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The official baseline score for Task 6 of the SamEval 2012 paper uses simple lexical features and reports a Pearson correlation of 0.311. \n",
    "\n",
    "Here is a table with the most significant results to consider:\n",
    "\n",
    "| Method/System        | Pearson Correlation | Description                              |\n",
    "|----------------------|----------------------|------------------------------------------|\n",
    "| SemEval Baseline     | 0.311               | Official baseline from SemEval Task 6, obtained from 'Table 1' of the SamEval 2012 paper   |\n",
    "| Top-Performing System| 0.8239              | Best result from SemEval Task 6, obtained by UKP-run2         |\n",
    "| 10th Participant     | 0.7562              | Threshold for excellent performance     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of Individual Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical Features\n",
    "Lexical features capture word-level semantic relationships between sentences using metrics such as shared synsets, lemma-based similarity, and Jaccard overlap across different parts of speech (e.g., nouns, verbs, adjectives). The primary features used include:\n",
    "\n",
    "- **Synset-based Similarity**: Measures such as shared synsets and maximum synset similarity.\n",
    "- **Lemma-based Features**: Metrics such as lemma diversity, proportion overlap, and lemma-based edit distance.\n",
    "\n",
    "To evaluate their effectiveness, we trained a **Random Forest model** separately on synset-based features, lemma-based features, and a combination of both. The results (averaged over 10 runs) are summarized below:\n",
    "\n",
    "| Feature Set   | Mean Pearson Correlation |\n",
    "|---------------|---------------------------|\n",
    "| Synset-based  | **0.7139**               |\n",
    "| Lemma-based   | **0.6688**               |\n",
    "| Combination   | **0.7416**               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntactic Features\n",
    "Syntactic features emphasize structural and grammatical aspects of sentences. \n",
    "\n",
    "These features focus on:\n",
    "- **Part-of-Speech (PoS) Counts**: Quantities of nouns, verbs, adjectives, and adverbs in each sentence.\n",
    "- **Jaccard Similarity**: Similarity in PoS distributions between sentences.\n",
    "\n",
    "In this case, we maintained one feature set. The results (averaged over 10 runs) are as follows:\n",
    "\n",
    "| Feature Set         | Mean Pearson Correlation |\n",
    "|---------------------|---------------------------|\n",
    "| Syntactic | **0.6846**               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Features Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined approach utilises both **lexical** and **syntactic features** to leverage the strengths of each. Lexical features capture semantic relationships at the word level, while syntactic features provide insight into structural alignment between sentences. This combination aims to address the limitations of each feature set when used independently.\n",
    "\n",
    "The performance of the combined feature set, evaluated using a **Random Forest model**, is summarized below (averaged over 10 runs):\n",
    "\n",
    "| Feature Set              | Mean Pearson Correlation |\n",
    "|--------------------------|---------------------------|\n",
    "| Lexical Features         | 0.7416               |\n",
    "| Syntactic Features       | 0.6846               |\n",
    "| Combined (Lexical + Syntactic) | **0.7672**               |\n",
    "\n",
    "#### Comparison of Individual Features\n",
    "The combined feature set achieved a Mean Pearson Correlation of 0.7672, surpassing the performance of both lexical-only (0.7416) and syntactic-only (0.6846) models. By integrating semantic similarity from lexical features and structural alignment from syntactic features, the model demonstrated improved robustness against diverse paraphrase types. This integration allowed the model to handle semantically equivalent sentences with different structures and structurally similar sentences with varying word usage more effectively.\n",
    "\n",
    "Despite this improvement, it's apparent that lexical features contribute the majority of the predictive power in this task. By utilising syntactic features during training of the model, the Pearson correlation improved by a very modest 0.0256.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Official SemEval Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-performing approach in this project achieved a **Mean Pearson Correlation of 0.7672** using a combination of lexical and syntactic features. This result is evaluated against the official SemEval 2012 Task 6 benchmarks:\n",
    "\n",
    "| System                  | Pearson Correlation | Description                                   |\n",
    "|-------------------------|----------------------|-----------------------------------------------|\n",
    "| SemEval Baseline        | 0.311               | Simple word overlap baseline.                |\n",
    "| UKP-run2 (Top System)   | 0.8239              | Best result using advanced features and ensemble techniques. |\n",
    "| SRIUBC-SYSTEM2          | 0.7562              | 10th-ranked system, representing high-performance threshold. |\n",
    "| Takelab-simple          | 0.8133              | Lightweight lexical and syntactic model.      |\n",
    "| UNT-CombinedRegression  | 0.7418              | Regression-based approach with feature combination. |\n",
    "| **Our Best Approach**   | **0.7672**          | Combined lexical and syntactic feature model.|\n",
    "\n",
    "#### **Comparison**\n",
    "Our best-performing approach significantly outperforms the SemEval baseline of **0.311**, demonstrating the effectiveness of incorporating both lexical and syntactic features. It also surpasses the **10th-ranked system (0.7562)**, placing it among the top-performing systems from the competition. Notably, it is competitive with other strong systems like **UNT-CombinedRegression (0.7418)** while falling slightly short of the **Takelab-simple (0.8133)** and **UKP-run2 (0.8239)** models.\n",
    "\n",
    "#### **Insights**\n",
    "The results show that our feature-driven approach is competitive with high-ranking systems in the SemEval task. While it does not surpass the top systems, it demonstrates the strength of a combined feature approach. Further exploration of advanced features, model tuning, or ensemble methods could narrow the gap with the top-performing systems. These results emphasize the value of balancing simplicity and complexity in feature engineering for sentence similarity tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF Mean Correlation by Feature Set\n",
    "<img src=\"../data/test/03_predicted/plots/rf_mean_correlation_by_feature_set.png\" alt=\"RF Mean Correlation\" width=\"800\"/>\n",
    "\n",
    "The graph illustrates the mean Pearson correlation achieved by the Random Forest model across different feature sets, including Original, Synsets, Lemma, Lexical, Syntactical, and All (combined). The combined feature set achieves the highest mean correlation (~0.767)\n",
    "\n",
    "\n",
    "\n",
    "#### RF RMSE by Feature Set\n",
    "<img src=\"../data/test/03_predicted/plots/rf_rmse_by_feature_set.png\" alt=\"RF RMSE\" width=\"800\"/>\n",
    "\n",
    "This bar chart presents the RMSE for each feature set, reflecting the model's prediction error. The combined feature set achieves the lowest RMSE, demonstrating its ability to minimize errors effectively. Lexical features also exhibit a relatively low RMSE, consistent with their strong performance in mean correlation. In comparison, syntactic and lemma features have higher RMSE values, indicating their limitations in accurately predicting similarity when used alone.\n",
    "\n",
    "\n",
    "\n",
    "#### RF Std Correlation by Feature Set\n",
    "<img src=\"../data/test/03_predicted/plots/rf_std_correlation_by_feature_set.png\" alt=\"RF Std Correlation\" width=\"800\"/>\n",
    "\n",
    "This graph displays the standard deviation of correlation values for each feature set. All feature sets display a very low standard deviation, with the combined feature set achieving the highest (~0.0035), indicating consistent and stable performance between runs for all feature sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project explored the effectiveness of various feature-based approaches for Semantic Textual Similarity (STS), focusing on lexical, syntactic, and combined feature sets. Using the Random Forest model, we evaluated the predictive power of these features in approximating human similarity scores.\n",
    "\n",
    "Key findings from the analysis include:\n",
    "\n",
    "- Lexical Features: Demonstrated strong performance, with a mean Pearson correlation of 0.7416, highlighting their ability to capture word-level semantics and explicit overlaps effectively.\n",
    "- Syntactic Features: Provided complementary insights into sentence structure, achieving a mean correlation of 0.6846 when combined with lexical features.\n",
    "- Combined Features: Achieved the best performance, with a mean correlation of 0.7672, showcasing the synergistic benefits of integrating lexical and syntactic perspectives.\n",
    "\n",
    "The results demonstrate that a feature-engineering approach can achieve competitive performance relative to benchmarks from the SemEval 2012 Task 6 competition. Our best model outperformed the SemEval baseline (0.311) and ranked above the 10th-place system (0.7562), underscoring the robustness of our feature combinations.\n",
    "\n",
    "While promising, the results also highlight areas for improvement. The limited gain from syntactic features suggests potential redundancy or inefficiencies in feature selection. Future work could focus on:\n",
    "\n",
    "- Selecting a more diverse set of features.\n",
    "- Train our own word embedding model (to remain within the constraints of the brief)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
