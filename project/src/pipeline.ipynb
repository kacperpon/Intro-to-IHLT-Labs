{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Textual Similarity Project\n",
    "\n",
    "**Authors**\n",
    "- Kacper Poniatowski\n",
    "- Pau Blanco\n",
    "\n",
    "TODO: Include brief outline of project\n",
    "- Context\n",
    "- What were trying to achieve\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Reqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\paubl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Force auto-reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from utils import load_data, evaluate_rf_model, drop_highly_correlated_features, update_results_csv, generate_plots_from_metrics\n",
    "from models import ModelTrainer\n",
    "from feature_extraction import FeatureExtractor\n",
    "from utils import save_predictions\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constant paths used throughout notebook\n",
    "TRAIN_PATH = '../data/train/01_raw/'\n",
    "TRAIN_GS_PATH = '../data/train/scores/'\n",
    "TEST_PATH = '../data/test/01_raw/'\n",
    "TEST_GS_PATH = '../data/test/scores/'\n",
    "TRAIN_SAVE_PATH = '../data/train/02_preprocessed/preprocessed_train_data.csv'\n",
    "TEST_SAVE_PATH = '../data/test/02_preprocessed/preprocessed_test_data.csv'\n",
    "PREDICTED_SAVE_PATH = '../data/test/03_predicted/'\n",
    "RESULTS_SAVE_PATH = \"project/test/03_predicted/results.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading train data\n",
      "\n",
      " Loading test data\n",
      "\n",
      " Train and test datasets loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "print('\\n Loading train data')\n",
    "all_train_files = ['SMTeuroparl', 'MSRvid', 'MSRpar']\n",
    "df_train = load_data(TRAIN_PATH, TRAIN_GS_PATH, all_train_files)\n",
    "\n",
    "# Load test data\n",
    "print('\\n Loading test data')\n",
    "all_test_files = ['SMTeuroparl', 'MSRvid', 'MSRpar', 'surprise.OnWN', 'surprise.SMTnews']\n",
    "df_test = load_data(TEST_PATH, TEST_GS_PATH, all_test_files)\n",
    "\n",
    "print('\\n Train and test datasets loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding POS based features...\n",
      "Adding synset-based features...\n",
      "Processed 2234 of 2234 rows (100%)      \n",
      "Adding lemma based features...\n",
      "Adding POS based features...\n",
      "Adding synset-based features...\n",
      "Processed 3108 of 3108 rows (100%)      \n",
      "Adding lemma based features...\n",
      "\n",
      " Features added to datasets successfully\n",
      "\n",
      " Train and test datasets saved as .csv files successfully\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Extract the desired features\n",
    "def add_features(dt):\n",
    "    feature_extractor.add_POS_statistics(dt)\n",
    "    feature_extractor.add_synset_statistics(dt)\n",
    "    feature_extractor.add_lemma_statistics(dt)\n",
    "\n",
    "# Add features to the training and test data\n",
    "add_features(df_train)\n",
    "add_features(df_test)\n",
    "\n",
    "print('\\n Features added to datasets successfully')\n",
    "\n",
    "# Save df_train and df_test to respective files - this is done to avoid re-running the\n",
    "# feature extraction process each time as synset extraction is computationally expensive\n",
    "df_test.to_csv(TEST_SAVE_PATH, index=False)\n",
    "df_train.to_csv(TRAIN_SAVE_PATH, index=False)\n",
    "\n",
    "print('\\n Train and test datasets saved as .csv files successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Score Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Existing Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train and test datasets loaded from .csv files successfully\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_SAVE_PATH is None or TEST_SAVE_PATH is None:\n",
    "    raise ValueError('TRAIN_SAVE_PATH and TEST_SAVE_PATH must be defined')\n",
    "\n",
    "if not os.path.exists(TRAIN_SAVE_PATH):\n",
    "    raise FileNotFoundError(f'The file {TRAIN_SAVE_PATH} does not exist')\n",
    "\n",
    "if not os.path.exists(TEST_SAVE_PATH):\n",
    "    raise FileNotFoundError(f'The file {TEST_SAVE_PATH} does not exist')\n",
    "\n",
    "original_df_train = pd.read_csv(TRAIN_SAVE_PATH)\n",
    "original_df_test = pd.read_csv(TEST_SAVE_PATH)\n",
    "\n",
    "print('\\n Train and test datasets loaded from .csv files successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           s1_n_words  s2_n_words  s1_n_verbs_tot  \\\n",
      "s1_n_words                   1.000000    0.918735        0.758300   \n",
      "s2_n_words                   0.918735    1.000000        0.673713   \n",
      "s1_n_verbs_tot               0.758300    0.673713        1.000000   \n",
      "s2_n_verbs_tot               0.590479    0.698453        0.740191   \n",
      "s1_n_verbs_pres              0.246101    0.191457        0.571911   \n",
      "...                               ...         ...             ...   \n",
      "lemma_lcs_length             0.814533    0.793255        0.545660   \n",
      "lemma_edit_distance          0.781976    0.809274        0.570023   \n",
      "proportion_s1_in_s2          0.202598    0.231090        0.077001   \n",
      "proportion_s2_in_s1          0.298470    0.213312        0.158630   \n",
      "lemma_position_similarity    0.153036    0.110382        0.118022   \n",
      "\n",
      "                           s2_n_verbs_tot  s1_n_verbs_pres  s2_n_verbs_pres  \\\n",
      "s1_n_words                       0.590479         0.246101         0.113169   \n",
      "s2_n_words                       0.698453         0.191457         0.203622   \n",
      "s1_n_verbs_tot                   0.740191         0.571911         0.352796   \n",
      "s2_n_verbs_tot                   1.000000         0.359068         0.585477   \n",
      "s1_n_verbs_pres                  0.359068         1.000000         0.583186   \n",
      "...                                   ...              ...              ...   \n",
      "lemma_lcs_length                 0.470067         0.036863        -0.021604   \n",
      "lemma_edit_distance              0.542890         0.157074         0.102533   \n",
      "proportion_s1_in_s2              0.086514        -0.158674        -0.147527   \n",
      "proportion_s2_in_s1              0.070091        -0.122421        -0.165168   \n",
      "lemma_position_similarity        0.040985         0.016534        -0.049000   \n",
      "\n",
      "                           s1_n_verbs_past  s2_n_verbs_past  s1_n_nouns  \\\n",
      "s1_n_words                        0.541747         0.366501    0.879111   \n",
      "s2_n_words                        0.477406         0.426594    0.812494   \n",
      "s1_n_verbs_tot                    0.616441         0.373132    0.520241   \n",
      "s2_n_verbs_tot                    0.473997         0.555110    0.405833   \n",
      "s1_n_verbs_pres                  -0.088747        -0.161231    0.066217   \n",
      "...                                    ...              ...         ...   \n",
      "lemma_lcs_length                  0.519421         0.441736    0.812901   \n",
      "lemma_edit_distance               0.443265         0.373852    0.741351   \n",
      "proportion_s1_in_s2               0.210567         0.230728    0.230298   \n",
      "proportion_s2_in_s1               0.269901         0.213446    0.330817   \n",
      "lemma_position_similarity         0.090492         0.038153    0.128458   \n",
      "\n",
      "                           s2_n_nouns  ...  max_lemma_similarity  \\\n",
      "s1_n_words                   0.822095  ...              0.229858   \n",
      "s2_n_words                   0.884512  ...              0.150631   \n",
      "s1_n_verbs_tot               0.481373  ...              0.147050   \n",
      "s2_n_verbs_tot               0.451101  ...              0.017157   \n",
      "s1_n_verbs_pres              0.045344  ...             -0.036966   \n",
      "...                               ...  ...                   ...   \n",
      "lemma_lcs_length             0.808128  ...              0.301991   \n",
      "lemma_edit_distance          0.758161  ...              0.067852   \n",
      "proportion_s1_in_s2          0.283861  ...              0.543419   \n",
      "proportion_s2_in_s1          0.239628  ...              0.546156   \n",
      "lemma_position_similarity    0.109522  ...              0.828971   \n",
      "\n",
      "                           lemma_jackard_similarity  shared_lemma_count  \\\n",
      "s1_n_words                                 0.224797            0.823098   \n",
      "s2_n_words                                 0.204095            0.802170   \n",
      "s1_n_verbs_tot                             0.093844            0.546626   \n",
      "s2_n_verbs_tot                             0.066700            0.463084   \n",
      "s1_n_verbs_pres                           -0.138852            0.036443   \n",
      "...                                             ...                 ...   \n",
      "lemma_lcs_length                           0.552054            0.965594   \n",
      "lemma_edit_distance                       -0.085678            0.576657   \n",
      "proportion_s1_in_s2                        0.941852            0.575162   \n",
      "proportion_s2_in_s1                        0.936890            0.599770   \n",
      "lemma_position_similarity                  0.394989            0.214088   \n",
      "\n",
      "                           dice_coefficient  lemma_bigram_overlap  \\\n",
      "s1_n_words                         0.264632              0.169512   \n",
      "s2_n_words                         0.236947              0.153650   \n",
      "s1_n_verbs_tot                     0.125389              0.060910   \n",
      "s2_n_verbs_tot                     0.085188              0.051548   \n",
      "s1_n_verbs_pres                   -0.144292             -0.128608   \n",
      "...                                     ...                   ...   \n",
      "lemma_lcs_length                   0.582465              0.479089   \n",
      "lemma_edit_distance               -0.042950             -0.110112   \n",
      "proportion_s1_in_s2                0.962305              0.731835   \n",
      "proportion_s2_in_s1                0.959543              0.738100   \n",
      "lemma_position_similarity          0.499750              0.201317   \n",
      "\n",
      "                           lemma_lcs_length  lemma_edit_distance  \\\n",
      "s1_n_words                         0.814533             0.781976   \n",
      "s2_n_words                         0.793255             0.809274   \n",
      "s1_n_verbs_tot                     0.545660             0.570023   \n",
      "s2_n_verbs_tot                     0.470067             0.542890   \n",
      "s1_n_verbs_pres                    0.036863             0.157074   \n",
      "...                                     ...                  ...   \n",
      "lemma_lcs_length                   1.000000             0.508607   \n",
      "lemma_edit_distance                0.508607             1.000000   \n",
      "proportion_s1_in_s2                0.542896            -0.061796   \n",
      "proportion_s2_in_s1                0.568452            -0.020979   \n",
      "lemma_position_similarity          0.263695            -0.094165   \n",
      "\n",
      "                           proportion_s1_in_s2  proportion_s2_in_s1  \\\n",
      "s1_n_words                            0.202598             0.298470   \n",
      "s2_n_words                            0.231090             0.213312   \n",
      "s1_n_verbs_tot                        0.077001             0.158630   \n",
      "s2_n_verbs_tot                        0.086514             0.070091   \n",
      "s1_n_verbs_pres                      -0.158674            -0.122421   \n",
      "...                                        ...                  ...   \n",
      "lemma_lcs_length                      0.542896             0.568452   \n",
      "lemma_edit_distance                  -0.061796            -0.020979   \n",
      "proportion_s1_in_s2                   1.000000             0.852209   \n",
      "proportion_s2_in_s1                   0.852209             1.000000   \n",
      "lemma_position_similarity             0.484480             0.473087   \n",
      "\n",
      "                           lemma_position_similarity  \n",
      "s1_n_words                                  0.153036  \n",
      "s2_n_words                                  0.110382  \n",
      "s1_n_verbs_tot                              0.118022  \n",
      "s2_n_verbs_tot                              0.040985  \n",
      "s1_n_verbs_pres                             0.016534  \n",
      "...                                              ...  \n",
      "lemma_lcs_length                            0.263695  \n",
      "lemma_edit_distance                        -0.094165  \n",
      "proportion_s1_in_s2                         0.484480  \n",
      "proportion_s2_in_s1                         0.473087  \n",
      "lemma_position_similarity                   1.000000  \n",
      "\n",
      "[79 rows x 79 columns]\n",
      "\n",
      " Total number of columns (features) in the correlation matrix: 79\n"
     ]
    }
   ],
   "source": [
    "# Drop first 4 columns to remove non-numerical columns\n",
    "df_train = original_df_train.drop(original_df_train.columns[:4], axis=1)\n",
    "df_test = original_df_test.drop(original_df_test.columns[:4], axis=1)\n",
    "\n",
    "correlation_matrix = df_train.corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Total number of columns in the correlation matrix\n",
    "total_cols = correlation_matrix.shape[1]\n",
    "print(f'\\n Total number of columns (features) in the correlation matrix: {total_cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Highly Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of features dropped: 32\n",
      "Remaining features after dropping highly correlated ones: Index(['s1_n_words', 's1_n_verbs_tot', 's2_n_verbs_tot', 's1_n_verbs_pres',\n",
      "       's2_n_verbs_pres', 's1_n_verbs_past', 's2_n_verbs_past',\n",
      "       's1_n_adjectives', 's1_n_adverbs', 's2_n_adverbs', 'dif_n_words',\n",
      "       'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past',\n",
      "       'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', 'jaccard_all_words',\n",
      "       'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives',\n",
      "       'jaccard_adverbs', 'all_all_shared_synsets_count',\n",
      "       'all_all_avg_synset_similarity', 'all_verb_shared_synsets_ratio',\n",
      "       'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
      "       'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity',\n",
      "       'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio',\n",
      "       'all_adj_max_synset_similarity', 'all_adv_shared_synsets_count',\n",
      "       'all_adv_shared_synsets_ratio', 'all_adv_max_synset_similarity',\n",
      "       'best_all_avg_synset_similarity', 'best_verb_shared_synsets_count',\n",
      "       'best_verb_max_synset_similarity', 'best_adj_shared_synsets_count',\n",
      "       'best_adj_avg_synset_similarity', 'best_adv_shared_synsets_count',\n",
      "       'best_adv_max_synset_similarity', 'avg_lemma_similarity',\n",
      "       'lemma_bigram_overlap', 'lemma_edit_distance', 'proportion_s2_in_s1',\n",
      "       'lemma_position_similarity'],\n",
      "      dtype='object')\n",
      "\n",
      "Number of remaining features after dropping highly correlated ones: 47\n",
      "\n",
      "Percentage of features removed: 40.51%\n",
      "\n",
      "Shape of df_test after column removal: (3108, 47)\n"
     ]
    }
   ],
   "source": [
    "# Initial number of columns\n",
    "total_cols = len(df_train.columns)\n",
    "\n",
    "# Find columns to drop from df_train and df_test\n",
    "dropped_columns = drop_highly_correlated_features(df_train)\n",
    "\n",
    "df_train_trimmed = df_train.drop(columns=dropped_columns)\n",
    "df_test_trimmed = df_test.drop(columns=dropped_columns)\n",
    "\n",
    "# Statistics\n",
    "print(f'\\nNumber of features dropped: {len(dropped_columns)}')\n",
    "print(\"Remaining features after dropping highly correlated ones:\", df_train_trimmed.columns)\n",
    "print(f'\\nNumber of remaining features after dropping highly correlated ones: {len(df_train_trimmed.columns)}')\n",
    "print(f'\\nPercentage of features removed: {(total_cols - len(df_train_trimmed.columns)) / total_cols * 100:.2f}%')\n",
    "\n",
    "# Print shape of df_test to verify consistency\n",
    "print(f'\\nShape of df_test after column removal: {df_test_trimmed.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Used Feature Sets (Prior to Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature sets for the analysis\n",
    "all_features = [\n",
    "            's1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_nouns', 's1_n_adjectives', 's1_n_adverbs', \n",
    "            's2_n_words', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_nouns', 's2_n_adjectives', 's2_n_adverbs', \n",
    "            'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', \n",
    "            'jaccard_all_words', 'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives', 'jaccard_adverbs',\n",
    "            \n",
    "            'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "\n",
    "            'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity',\n",
    "            'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity',\n",
    "            'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity',\n",
    "            'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity',\n",
    "            'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity',\n",
    "\n",
    "            'lemma_diversity', 'shared_lemmas_ratio', 'lemma_jackard_similarity', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient',\n",
    "            'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity'\n",
    "            ]\n",
    "\n",
    "PoS_features = [\n",
    "            's1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_nouns', 's1_n_adjectives', 's1_n_adverbs', \n",
    "            's2_n_words', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_nouns', 's2_n_adjectives', 's2_n_adverbs', \n",
    "            'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', \n",
    "            'jaccard_all_words', 'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives', 'jaccard_adverbs',\n",
    "            ]\n",
    "\n",
    "synset_features = [\n",
    "            'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "\n",
    "            'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity',\n",
    "            'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity',\n",
    "            'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity',\n",
    "            'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity',\n",
    "            'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity',\n",
    "            ]\n",
    "\n",
    "lemma_features = [\n",
    "            'lemma_diversity', 'shared_lemmas_ratio', 'lemma_jackard_similarity', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient',\n",
    "            'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity'\n",
    "            ]\n",
    "\n",
    "lexical_features = [\n",
    "            'all_all_shared_synsets_count', 'all_all_shared_synsets_ratio', 'all_all_avg_synset_similarity', 'all_all_max_synset_similarity',\n",
    "            'all_verb_shared_synsets_count', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity',\n",
    "            'all_noun_shared_synsets_count', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_noun_max_synset_similarity',\n",
    "            'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_avg_synset_similarity', 'all_adj_max_synset_similarity',\n",
    "            'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_avg_synset_similarity', 'all_adv_max_synset_similarity',\n",
    "\n",
    "            'best_all_shared_synsets_count', 'best_all_shared_synsets_ratio', 'best_all_avg_synset_similarity', 'best_all_max_synset_similarity',\n",
    "            'best_verb_shared_synsets_count', 'best_verb_shared_synsets_ratio', 'best_verb_avg_synset_similarity', 'best_verb_max_synset_similarity',\n",
    "            'best_noun_shared_synsets_count', 'best_noun_shared_synsets_ratio', 'best_noun_avg_synset_similarity', 'best_noun_max_synset_similarity',\n",
    "            'best_adj_shared_synsets_count', 'best_adj_shared_synsets_ratio', 'best_adj_avg_synset_similarity', 'best_adj_max_synset_similarity',\n",
    "            'best_adv_shared_synsets_count', 'best_adv_shared_synsets_ratio', 'best_adv_avg_synset_similarity', 'best_adv_max_synset_similarity',\n",
    "\n",
    "            'lemma_diversity', 'shared_lemmas_ratio', 'lemma_jackard_similarity', 'avg_lemma_similarity', 'max_lemma_similarity', 'shared_lemma_count', 'dice_coefficient',\n",
    "            'lemma_bigram_overlap', 'lemma_lcs_length', 'lemma_edit_distance', 'proportion_s1_in_s2', 'proportion_s2_in_s1', 'lemma_position_similarity'\n",
    "]\n",
    "\n",
    "# Organize feature sets into labeled tuples\n",
    "features_sets = [\n",
    "    ('All', all_features),\n",
    "    ('Synsets', synset_features),\n",
    "    ('Lemmas', lemma_features),\n",
    "    ('PoS (Syntactic)', PoS_features),\n",
    "    ('Lexical', lexical_features),\n",
    "]\n",
    "\n",
    "# File sets for the analysis\n",
    "files_sets = [\n",
    "    ('SMTeuroparl', ['SMTeuroparl'], ['SMTeuroparl', 'surprise.OnWN', 'surprise.SMTnews']),\n",
    "    ('MSRvid', ['MSRvid'], ['MSRvid', 'surprise.OnWN', 'surprise.SMTnews']),\n",
    "    ('MSRpar', ['MSRpar'], ['MSRpar', 'surprise.OnWN', 'surprise.SMTnews']),\n",
    "    ('All', ['SMTeuroparl', 'MSRvid', 'MSRpar'], ['SMTeuroparl', 'MSRvid', 'MSRpar', 'surprise.OnWN', 'surprise.SMTnews'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: 47 features\n",
      "['s1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_adjectives', 's1_n_adverbs', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_adverbs', 'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', 'jaccard_all_words', 'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives', 'jaccard_adverbs', 'all_all_shared_synsets_count', 'all_all_avg_synset_similarity', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_max_synset_similarity', 'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_max_synset_similarity', 'best_all_avg_synset_similarity', 'best_verb_shared_synsets_count', 'best_verb_max_synset_similarity', 'best_adj_shared_synsets_count', 'best_adj_avg_synset_similarity', 'best_adv_shared_synsets_count', 'best_adv_max_synset_similarity', 'avg_lemma_similarity', 'lemma_bigram_overlap', 'lemma_edit_distance', 'proportion_s2_in_s1', 'lemma_position_similarity']\n",
      "PoS (Syntactic): 22 features\n",
      "['s1_n_words', 's1_n_verbs_tot', 's1_n_verbs_pres', 's1_n_verbs_past', 's1_n_adjectives', 's1_n_adverbs', 's2_n_verbs_tot', 's2_n_verbs_pres', 's2_n_verbs_past', 's2_n_adverbs', 'dif_n_words', 'dif_n_verbs_tot', 'dif_n_verbs_pres', 'dif_n_verbs_past', 'dif_n_nouns', 'dif_n_adjectives', 'dif_n_adverbs', 'jaccard_all_words', 'jaccard_verbs', 'jaccard_nouns', 'jaccard_adjectives', 'jaccard_adverbs']\n",
      "Synsets: 20 features\n",
      "['all_all_shared_synsets_count', 'all_all_avg_synset_similarity', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_max_synset_similarity', 'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_max_synset_similarity', 'best_all_avg_synset_similarity', 'best_verb_shared_synsets_count', 'best_verb_max_synset_similarity', 'best_adj_shared_synsets_count', 'best_adj_avg_synset_similarity', 'best_adv_shared_synsets_count', 'best_adv_max_synset_similarity']\n",
      "Lemmas: 5 features\n",
      "['avg_lemma_similarity', 'lemma_bigram_overlap', 'lemma_edit_distance', 'proportion_s2_in_s1', 'lemma_position_similarity']\n",
      "Lexical: 25 features\n",
      "['all_all_shared_synsets_count', 'all_all_avg_synset_similarity', 'all_verb_shared_synsets_ratio', 'all_verb_avg_synset_similarity', 'all_verb_max_synset_similarity', 'all_noun_shared_synsets_ratio', 'all_noun_avg_synset_similarity', 'all_adj_shared_synsets_count', 'all_adj_shared_synsets_ratio', 'all_adj_max_synset_similarity', 'all_adv_shared_synsets_count', 'all_adv_shared_synsets_ratio', 'all_adv_max_synset_similarity', 'best_all_avg_synset_similarity', 'best_verb_shared_synsets_count', 'best_verb_max_synset_similarity', 'best_adj_shared_synsets_count', 'best_adj_avg_synset_similarity', 'best_adv_shared_synsets_count', 'best_adv_max_synset_similarity', 'avg_lemma_similarity', 'lemma_bigram_overlap', 'lemma_edit_distance', 'proportion_s2_in_s1', 'lemma_position_similarity']\n"
     ]
    }
   ],
   "source": [
    "# Pull remaining feature column names from df into \"all_features\"\n",
    "remaining_features = df_train_trimmed.columns.tolist()\n",
    "\n",
    "def filter_features(feature_set, remaining_features):\n",
    "    return [f for f in feature_set if f in remaining_features]\n",
    "\n",
    "# Create new filtered feature groups\n",
    "new_all_features = filter_features(all_features, remaining_features)\n",
    "new_PoS_features = filter_features(PoS_features, remaining_features)\n",
    "new_synset_features = filter_features(synset_features, remaining_features)\n",
    "new_lemma_features = filter_features(lemma_features, remaining_features)\n",
    "new_lexical_features = filter_features(lexical_features, remaining_features)\n",
    "\n",
    "filtered_feature_sets = [\n",
    "    ('All', new_all_features),\n",
    "    ('PoS (Syntactic)', new_PoS_features),\n",
    "    ('Synsets', new_synset_features),\n",
    "    ('Lemmas', new_lemma_features),\n",
    "    ('Lexical', new_lexical_features),\n",
    "]\n",
    "\n",
    "# Display remaining features in each feature set\n",
    "for name, features in filtered_feature_sets:\n",
    "    print(f\"{name}: {len(features)} features\")\n",
    "    print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the first 4 columns from df_train to df at the beginning\n",
    "df_train_trimmed = pd.concat([original_df_train[original_df_train.columns[:4]], df_train_trimmed], axis=1)\n",
    "df_test_trimmed = pd.concat([original_df_test[original_df_test.columns[:4]], df_test_trimmed], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer()\n",
    "N_ITERATIONS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Lexical Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Pearson Correlation Score using RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7420479435809793\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7428309217471607\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_lex\\2024-12-11_08-47-12_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_lex\\2024-12-11_08-47-12_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_lex\\2024-12-11_08-47-12_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "best_rf_model_lex, rf_params_lex, metrics_lex, mean_correlation_rf_lex = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    df_train_trimmed, \n",
    "    df_test_trimmed, \n",
    "    new_lexical_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_lex\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=f\"{PREDICTED_SAVE_PATH}/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"lexical\",\n",
    "    metrics=metrics_lex,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_lex.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Syntactic Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Pearson Correlation Score using RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.6835064595393991\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.6840284840485038\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_pos\\2024-12-11_00-09-50_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_pos\\2024-12-11_00-09-50_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_pos\\2024-12-11_00-09-50_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "best_rf_model_pos, rf_params_pos, metrics_pos, mean_correlation_rf_pos = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    df_train_trimmed, \n",
    "    df_test_trimmed, \n",
    "    new_PoS_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_pos\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=\"project/test/03_predicted/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"POS\",\n",
    "    metrics=metrics_pos,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_pos.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Lexical + Syntactic Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Pearson Correlation Score using RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.7620201098570732\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7612425130532474\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_lex_pos\\2024-12-11_00-10-46_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_lex_pos\\2024-12-11_00-10-46_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_lex_pos\\2024-12-11_00-10-46_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "best_rf_model_lex_pos, rf_params_lex_pos, metrics_lex_pos, mean_correlation_rf_lex_pos = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    df_train_trimmed, \n",
    "    df_test_trimmed, \n",
    "    new_lexical_features + new_PoS_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_lex_pos\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=\"project/test/03_predicted/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"Lexical+POS\",\n",
    "    metrics=metrics_lex_pos,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_lex_pos.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Additional Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Pearson Correlation Score using RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Pearson correlation for the best RF model: 0.761241689964608\n",
      "Computing mean Pearson correlation for 10 iterations...\n",
      "Mean Pearson correlation over 10 iterations: 0.7630784866609258\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_rf_all\\2024-12-11_00-12-16_test_data.csv\n",
      "Feature importance graph saved as: ..\\data\\test\\03_predicted\\predicted_rf_all\\2024-12-11_00-12-16_feature_importance.png\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_rf_all\\2024-12-11_00-12-16_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "best_rf_model_all, rf_params_all, metrics_rf_all, mean_correlation_rf_all = evaluate_rf_model(\n",
    "    model_trainer, \n",
    "    df_train_trimmed, \n",
    "    df_test_trimmed, \n",
    "    new_all_features, \n",
    "    'gs', \n",
    "    PREDICTED_SAVE_PATH,\n",
    "    \"predicted_rf_all\"\n",
    ")\n",
    "\n",
    "update_results_csv(\n",
    "    results_file=\"project/test/03_predicted/results.csv\",\n",
    "    model_name=\"RandomForest\",\n",
    "    feature_set=\"All\",\n",
    "    metrics=metrics_rf_all,\n",
    "    prediction_file=f\"{PREDICTED_SAVE_PATH}/predicted_rf_all.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots have been saved to ../data/test/03_predicted//plots/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_plots_from_metrics(f\"{PREDICTED_SAVE_PATH}/results.csv\", save_path=f\"{PREDICTED_SAVE_PATH}/plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Additional Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'batch_size': 32, 'epochs': 100, 'model__hidden_layers': 2, 'model__learning_rate': 0.001, 'model__neurons': 10}\n",
      "\n",
      " Pearson correlation for the best NN model: 0.7314233557687264\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_nn_all.csv\\2024-12-11_08-47-54_test_data.csv\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_nn_all.csv\\2024-12-11_08-47-54_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Train the best NN model using all features\n",
    "best_nn_model = model_trainer.train_NN(df_train_trimmed, new_all_features, 'gs')\n",
    "\n",
    "# Predict the test data\n",
    "df_test_trimmed['predicted_nn'] = best_nn_model.predict(df_test_trimmed[new_all_features])\n",
    "\n",
    "# Calculate the Pearson correlation for the best NN model\n",
    "correlation_nn = pearsonr(df_test_trimmed['gs'], df_test_trimmed['predicted_nn'])[0]\n",
    "print(f'\\n Pearson correlation for the best NN model: {correlation_nn}')\n",
    "\n",
    "# Save the predictions\n",
    "save_predictions(df_test, PREDICTED_SAVE_PATH, 'predicted_nn_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "Best Hyperparameters: {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "\n",
      " Pearson correlation for the best MLP model: 0.702167870890418\n",
      "Predicted data saved to CSV: ..\\data\\test\\03_predicted\\predicted_mlp_all.csv\\2024-12-11_08-48-38_test_data.csv\n",
      "Predicted data saved to Excel: ..\\data\\test\\03_predicted\\predicted_mlp_all.csv\\2024-12-11_08-48-38_test_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Train the best MLP model using all features\n",
    "best_mlp_model = model_trainer.train_MLP(df_train_trimmed, new_all_features, 'gs')\n",
    "\n",
    "# Predict the test data\n",
    "df_test_trimmed['predicted_mlp'] = best_mlp_model.predict(df_test_trimmed[new_all_features])\n",
    "\n",
    "# Calculate the Pearson correlation for the best NN model\n",
    "correlation_mlp = pearsonr(df_test_trimmed['gs'], df_test_trimmed['predicted_mlp'])[0]\n",
    "print(f'\\n Pearson correlation for the best MLP model: {correlation_mlp}')\n",
    "\n",
    "# Save the predictions\n",
    "save_predictions(df_test, PREDICTED_SAVE_PATH, 'predicted_mlp_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Code (TODO: Discuss if we're keeping this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare correlation based on file and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All SMTeuroparl 0.5260762425283135\n",
      "All SMTeuroparl 0.6321408221170948 (with surprise files)\n",
      "Synsets SMTeuroparl 0.5089847580757192\n",
      "Synsets SMTeuroparl 0.5984333819281736 (with surprise files)\n",
      "Lemmas SMTeuroparl 0.48728005311908856\n",
      "Lemmas SMTeuroparl 0.5838403729608709 (with surprise files)\n",
      "PoS (Syntactical) SMTeuroparl 0.28082396513890506\n",
      "PoS (Syntactical) SMTeuroparl 0.3062866488678532 (with surprise files)\n",
      "Lexical SMTeuroparl 0.5317844394052379\n",
      "Lexical SMTeuroparl 0.6256457418945189 (with surprise files)\n",
      "\n",
      "All MSRvid 0.8611804247098516\n",
      "All MSRvid 0.7915275196347473 (with surprise files)\n",
      "Synsets MSRvid 0.8597524788987414\n",
      "Synsets MSRvid 0.7654246344934643 (with surprise files)\n",
      "Lemmas MSRvid 0.6572899927033499\n",
      "Lemmas MSRvid 0.6420521699471147 (with surprise files)\n",
      "PoS (Syntactical) MSRvid 0.20808034566065797\n",
      "PoS (Syntactical) MSRvid 0.35248343346031863 (with surprise files)\n",
      "Lexical MSRvid 0.8620913345693634\n",
      "Lexical MSRvid 0.7776658207000915 (with surprise files)\n",
      "\n",
      "All MSRpar 0.5980055133354054\n",
      "All MSRpar 0.5228975085251519 (with surprise files)\n",
      "Synsets MSRpar 0.5177578550169951\n",
      "Synsets MSRpar 0.4476150310809409 (with surprise files)\n",
      "Lemmas MSRpar 0.5530242136102264\n",
      "Lemmas MSRpar 0.5424651769788617 (with surprise files)\n",
      "PoS (Syntactical) MSRpar 0.3351401615843118\n",
      "PoS (Syntactical) MSRpar 0.020682043233786286 (with surprise files)\n",
      "Lexical MSRpar 0.5900589694756582\n",
      "Lexical MSRpar 0.5382029182207234 (with surprise files)\n",
      "\n",
      "All All 0.8032769795343603\n",
      "All All 0.7541890174149117 (with surprise files)\n",
      "Synsets All 0.7835546046102503\n",
      "Synsets All 0.7242258621409416 (with surprise files)\n",
      "Lemmas All 0.6935845573569963\n",
      "Lemmas All 0.6414015564547721 (with surprise files)\n",
      "PoS (Syntactical) All 0.4763025071936748\n",
      "PoS (Syntactical) All 0.4546740328382729 (with surprise files)\n",
      "Lexical All 0.8032998452799843\n",
      "Lexical All 0.7456461981167642 (with surprise files)\n"
     ]
    }
   ],
   "source": [
    "N_ITERS = 10\n",
    "# Train a Random Forest for each feature set and each file set\n",
    "for t_name, tr_set, vl_set in files_sets:\n",
    "    print()\n",
    "    for f_name, f_set in features_sets:\n",
    "        train = df_train[df_train['file'].isin(tr_set)]\n",
    "        \n",
    "        test = df_test[df_test['file'].isin(tr_set)]\n",
    "        corr = 0\n",
    "        for i in range(N_ITERS):\n",
    "            model = train_single_RF(train, f_set, 'gs', params)\n",
    "            test.loc[:, 'predicted'] = model.predict(test[f_set])\n",
    "            corr += pearsonr(test['gs'], test['predicted'])[0]\n",
    "        print(f_name, t_name, corr / N_ITERS)\n",
    "\n",
    "        test = df_test[df_test['file'].isin(vl_set)]\n",
    "        corr = 0\n",
    "        for i in range(N_ITERS):\n",
    "            model = train_single_RF(train, f_set, 'gs', params)\n",
    "            test.loc[:, 'predicted'] = model.predict(test[f_set])\n",
    "            corr += pearsonr(test['gs'], test['predicted'])[0]\n",
    "        print(f_name, t_name, corr / N_ITERS, '(with surprise files)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is it possible to predict separating by source file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in predicting file: 0.8249106687085248\n",
      "[2 2 2 ... 0 0 0]\n",
      "Number of rows in partition  0 : 750\n",
      "One model\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\MAI\\IHLT\\Intro-to-IHLT-Labs\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Number of rows in partition  1 : 750\n",
      "One model\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best Hyperparameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Number of rows in partition  2 : 734\n",
      "One model\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best Hyperparameters: {'bootstrap': True, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Predicted len:  1454\n",
      "Predicted len:  1354\n",
      "Predicted len:  300\n",
      "Final correlation:  0.7283554075731684\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Get a filtered test set\n",
    "filt_df_test = df_test[df_test['file'].isin(all_train_files)].copy()\n",
    "\n",
    "# Encode the file column to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_train['file_encoded'] = label_encoder.fit_transform(df_train['file'])\n",
    "filt_df_test.loc[:, 'file_encoded'] = label_encoder.transform(filt_df_test['file'])\n",
    "\n",
    "# On the train set, do a categorical encoding for the file column\n",
    "y_train = to_categorical(df_train['file_encoded'], num_classes=len(all_train_files))\n",
    "# Filter the test dataset and do the categorical encoding\n",
    "filt_y_test = to_categorical(filt_df_test['file_encoded'], num_classes=len(all_train_files))\n",
    "\n",
    "# Create a random forest classification model from df_train to y_train\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(df_train[all_features], y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(filt_df_test[all_features])\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = np.mean(np.argmax(filt_y_test, axis=1) == np.argmax(y_pred, axis=1))\n",
    "print('Accuracy in predicting file:', accuracy)\n",
    "\n",
    "# In the global train_set, assign the predicted file classç\n",
    "print(np.argmax(clf.predict(df_train[all_features]), axis=1))\n",
    "df_train['pred_file'] = np.argmax(clf.predict(df_train[all_features]), axis=1)\n",
    "\n",
    "# Train a regression random forest for each partition of df_train based on pred_file\n",
    "partitioned_models = []\n",
    "for file_class in range(len(all_train_files)):\n",
    "    partition = df_train[df_train['pred_file']== file_class]\n",
    "    print(\"Number of rows in partition \", file_class, \":\", partition.shape[0])\n",
    "    if not partition.empty:\n",
    "        print(\"One model\")\n",
    "        model, params = train_RF(partition, all_features, 'gs')\n",
    "        # model = RandomForestRegressor(**params)\n",
    "        # model.fit(partition[all_features], partition['gs'])\n",
    "        partitioned_models.append(model)\n",
    "\n",
    "# For each row in df_test, predict the file class and use the corresponding model to predict the predicted_gs\n",
    "# Predict the file class for each row in df_test\n",
    "df_test['pred_file'] = np.argmax(clf.predict(df_test[all_features]), axis=1)\n",
    "\n",
    "# For each partition of df_test based on pred_file, use the corresponding model to predict the gs\n",
    "for fcls in range(len(all_train_files)):\n",
    "    pred_gs = partitioned_models[fcls].predict(df_test[df_test['pred_file'] == fcls][all_features])\n",
    "    print(\"Predicted len: \", len(pred_gs))\n",
    "    df_test.loc[df_test['pred_file'] == fcls, 'gs_predicted'] = pred_gs\n",
    "\n",
    "# Compute the pearson correlation\n",
    "final_corr = pearsonr(df_test['gs'], df_test['gs_predicted'])[0]\n",
    "print(\"Final correlation: \", final_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result analysis to go here. This could go into a separate file (notebook or .md file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion to go here. This could go into a separate file (notebook or .md file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
